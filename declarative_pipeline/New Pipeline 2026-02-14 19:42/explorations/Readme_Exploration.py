# Databricks notebook source
# MAGIC %md
# MAGIC ### Example Exploratory Notebook
# MAGIC
# MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
# MAGIC
# MAGIC **Note**: This notebook is not executed as part of the pipeline.

# COMMAND ----------

# MAGIC %md
# MAGIC | Feature               | Materialized View<br>(@pipelines.table + `read`)           | Streaming Table<br>(@pipelines.table + `readStream`) |
# MAGIC | --------------------- | ---------------------------------------------------------- | ---------------------------------------------------- |
# MAGIC | **Data Processing**   | Recomputes full logical result from latest source snapshot | Processes only new/changed data incrementally        |
# MAGIC | **Efficiency**        | Best for moderate data and complex transformations         | Best for high-volume continuous ingestion            |
# MAGIC | **Latency**           | Batch / Scheduled                                          | Near real-time / Continuous                          |
# MAGIC | **Deletes / Updates** | Reflects source only if source is snapshot-consistent      | Needs `APPLY CHANGES` for correct upserts/deletes    |
# MAGIC | **CDC Support**       | ❌ No native CDC                                            | ✅ With `APPLY CHANGES`                               |
# MAGIC | **State Management**  | Rebuilt on each run (logically)                            | Maintains state via checkpoints                      |
# MAGIC | **Cost Pattern**      | Higher compute for large datasets                          | Lower cost for large streams                         |
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC Both materialized and streaming DLT tables are stored as Delta Lake tables using Parquet files and transaction logs. The difference from normal Delta tables is not in storage format but in management—DLT automatically handles orchestration, lineage, quality, and refresh.
# MAGIC - Physically ❌ → NO DIFFERENCE
# MAGIC - Logically layer ✅ → BIG DIFFERENCE

# COMMAND ----------

# MAGIC %md
# MAGIC **spark.readStream.table** This creates a Streaming Delta Live Table(STREAMING_TABLE).
# MAGIC - ✔ Source is read as a stream
# MAGIC - ✔ Target is a managed DLT Delta table
# MAGIC - ✔ Data is processed incrementally
# MAGIC - ✔ Checkpoints are maintained automatically
# MAGIC 1. Declarative Programming
# MAGIC    - We define table logic using decorators.
# MAGIC    - DLT manages execution and dependencies.
# MAGIC
# MAGIC 2. Streaming Ingestion
# MAGIC    -    a. Source is read incrementally using readStream.
# MAGIC    -    b. Target table receives new records automatically.
# MAGIC    -    c. By default, only inserts are handled.
# MAGIC    -    d. Updates and deletes require APPLY CHANGES (CDC).

# COMMAND ----------

# MAGIC %md
# MAGIC #spark.read():
# MAGIC - Reads full snapshot
# MAGIC - Reflects inserts/updates/deletes
# MAGIC - No incremental processing
# MAGIC
# MAGIC #spark.readStream():
# MAGIC - Incremental ingestion
# MAGIC - Handles inserts only by default
# MAGIC - Updates/deletes need CDC + APPLY CHANGES

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from gcp_mysql_fc_wd36.logistics.shipments1

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from catalog2_we47.schema2_we47.silver_shipmentsDLT1

# COMMAND ----------


#Pure Pyspark DSL (Imperative because I didn't used decorator of pipelines offerred by Databricks) 
def load_data_bronze_imp_dp():#Imperative program
    df1=spark.read.table("gcp_mysql_fc_wd36.logistics.shipments1")
    df2=df1.filter("city is null")
    return df2

load_data_bronze_imp_dp().show(200)
#load_data_bronze_imp_dp().write.saveAsTable("lakehousecat.default.shipment1_bronze")

# COMMAND ----------

from pyspark import pipelines

@pipelines.table(name="catalog2_we47.schema2_we47.silver_shipmentsDLT1")#it becomes declarative by specifying decorator on top of the function
def load_data_bronze_imp_dp():#Imperative program
    df1=spark.readStream.table("gcp_mysql_fc_wd36.logistics.shipments1")#readStream will only
    df2=df1.filter("city is null")
    return df2

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from gcp_mysql_fc_wd36.logistics.shipments1

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from catalog2_we47.schema2_we47.silver_shipmentsDLT1
