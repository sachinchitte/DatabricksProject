{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca193f2-8bbd-46aa-9320-964abe63305d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6b23ae-2025-4823-8e73-3b78d65e34bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data munging is the process of converting raw data into a usable format by cleaning, transforming, and enriching it. ![](/Workspace/Users/sachinchitte4@gmail.com/Pratice/stage1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672d1231-c742-4a26-a5da-40b4bba311ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Passive Data Munging : Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns.**\n",
    "1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality\n",
    "- Null columns are there\n",
    "- duplicate rows\n",
    "- format issues are there (age is not in number format eg. 7-7)\n",
    "- Uniformity issues (Artist, artist)\n",
    "- Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b32287-64b6-4cca-8ff1-874d91e8dd36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbbb281-d1cb-4aa2-bbed-c10e4f32b1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists BreadandButter; \n",
    "create database if not exists BreadandButter.Data_Ingestion_DB; \n",
    "create volume if not exists BreadandButter.Data_Ingestion_DB.Data_DE_VL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d13eef-2b5e-4531-91ef-7587222b7b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf0d154-cbfd-441a-9249-7ced7c87ea6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))\n",
    "#inferSchema=True ->Spark scans data and assigns data types automatically\n",
    "#sample(.1) #Randomly selects ~10% of rows -> Data inspection\n",
    "#take(20) -> Displaying first 20 rows (Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93bfeb7-0b1f-40f1-81a2-2881252e18e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "rawdf1.printSchema() #I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "\n",
    "#Column names in exact order\n",
    "print(rawdf1.columns) #I am understanding the column numbers/order and the column names\n",
    "\n",
    "#Column name + datatype mapping\n",
    "print(rawdf1.dtypes) #Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81f9193-a27b-4484-b9ed-bd9ea10d8397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Identifying all string columns (dynamic logic)\n",
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0])\n",
    "\n",
    "#Full structural metadata\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efa4159-f6dc-4805-b03b-9b731789ac3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "distinct() and dropDuplicates() both remove duplicate rows across all columns when no subset is provided. \n",
    "- The key difference is that dropDuplicates() allows deduplication based on specific columns, making it more flexible for data engineering use cases.   ex: rawdf1.dropDuplicates(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42a22a9-f31a-424f-a982-59f6f9c61c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "\n",
    "#Total row count (baseline)\n",
    "print(\"actual count of the data\",rawdf1.count()) \n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(SQL-style operation)\n",
    "print(\"de-duplicated record (all columns) count sqlstyle\",rawdf1.distinct().count())\n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(DataFrame-specific API)\n",
    "print(\"de-duplicated record (all columns) count DF api\",rawdf1.dropDuplicates().count())\n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(remove duplicates based on specific columns)\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())\n",
    "\n",
    "#describe() provides basic statistics like count, mean, min, and max, \n",
    "display(rawdf1.describe())\n",
    "\n",
    "# while summary() extends this by adding percentile-based distribution metrics such as median and quartiles, making it more suitable for deeper data quality analysis.\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7314c5c-36d3-49ac-bfc8-f08658adef5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Active Data Munging** is the continuous process of structuring, validating, cleansing, scrubbing, deduplicating, and standardizing evolving data to make it analytics-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0566832-8227-4d77-899b-713af38b3bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Combining Data + Schema Evolution/Merging (Structuring)\n",
    "- Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "- De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801cd94c-0d39-473a-afe6-07b74d3b964e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1)**Questions related to multiple files/paths/sub path handling**\n",
    "-->I have data in different filenames in a single/multiple location, i need to read all these data in a df - path=[\"path1/file1\",\"path1/file2\",\"path2/file3\"] I have data in single pattern of file names in a single/multiple locations or subfolders, i need to read all these data in a df - path=[\"path1/\",\"path1/\",\"path2/\"], pathGlobFilter=\"custsm*\", recursiveFileLookup=True\n",
    "\n",
    "2)**Questions related handling evolving data structure with data ingested in different days/periods - Ans. Schema Evolution**\n",
    "Evolution is growth over the time (Filesystem level).. Eg. Source is sending data with additional columns week over week in csv format\n",
    "1. Read and write in Serialized format( ORC,Parquet)\n",
    "2. Read DF with mergeSchema = True\n",
    "\n",
    "3)**Questions related handling data from different sources with different related structure in a same day - Ans. Schema Merging/Melting (Dataframe level)**    Eg. Source1 is sending custsmodified_NY with 5 columns and Source2 is sending custsmodified TX with 4 columns\n",
    "1. Read file1 in DF1, read file2 in DF2\n",
    "2. Create DF3 by merging DF1 and DF2 using df1.unionByName(df2,allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2410378d-1cc7-436b-bbba-347c97802566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified\") \n",
    "#2. Multiple files (with different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/\",\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c1b09f-41f6-45c5-a00a-8f7d933f3a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- When you go for Schema Merging/Melting and Schema Evolution?\n",
    "- Schema Merging/Melting (unionByName,allowMissingColumns)- If we get multiple files\n",
    "- Schema Evolution (orc/parquet with mergeSchema) - If no. of columns are keeps added by the source system\n",
    "- when we know structure of the file already - schema merge/ schema not known earlier  - schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe161708-ca16-4429-a9b4-5cb4f2f3c27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Schema Evolution:\n",
    "Handling schema changes over time in the same dataset.\n",
    "\n",
    "Schema Merging:\n",
    "Combining different schemas from multiple sources into one structure.\n",
    "\n",
    "- If multiple files with different structures arrive together → Schema Merging.\n",
    "- If the same source keeps adding columns over time → Schema Evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec52906-d80d-4b1f-9c4c-95b1d82e8bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **1. Combining Data + Schema Evolution/Merging (Structuring)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eee3430-c77c-441d-9a82-216ca7638f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema Merging**\n",
    "- Schema Merging is the process of combining data from multiple sources with different schemas at the same point in time into a single unified structure.\n",
    "\n",
    "**Scope**\n",
    "- Same-day / same batch\n",
    "- DataFrame level\n",
    "- _Typical Scenario_\n",
    "- Source A (NY): id, name, email\n",
    "- Source B (TX): id, name, phone\n",
    "\n",
    "**_How it is handled_**\n",
    "- Read separately\n",
    "- Merge using unionByName\n",
    "- df_all = df_ny.unionByName(df_tx, allowMissingColumns=True)\n",
    "\n",
    "\n",
    "What Spark does\n",
    "- Matches columns by name\n",
    "- Adds missing columns as NULL\n",
    "- Produces unified DataFrame\n",
    "\n",
    "Key Point\n",
    "- Schema Merging happens because sources differ, not because time changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0976b393-ec41-428d-a622-62afd9420e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(Important interview question also as like schema evolution...)\n",
    "#4. Multiple files with different structure in multiple paths or sub paths\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf1)\n",
    "display(rawdf2)\n",
    "\n",
    "rawdf_merged=rawdf1.union(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype....Union is position-based ,Same column order, Same number of columns, Same data types.\n",
    "display(rawdf_merged)\n",
    "\n",
    "#Expected right approach to follow #allowMissingColumns=True -> Adds missing columns with NULL\n",
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True) #In unionByName -> Columns matched by name # Missing columns → NULL\n",
    "display(rawdf_merged) #done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d29844d-1114-4e16-8c88-f2ea144f7480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here, we are merging two files because both are in CSV format. If one file is CSV and the other file is in a different format, what should we do in this scenario? it will be handled automatically\n",
    "#rawdf2.write.json(\"/Volumes/workspace/wd36schema/ingestion_volume/staging/csvjson\")\n",
    "rawdf3=spark.read.json(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/simple_json.txt\")\n",
    "rawdf_merged=rawdf_merged.unionByName(rawdf3,allowMissingColumns=True)\n",
    "display(rawdf_merged)#Expected dataframe to proceed further munging on a single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c27fcc-773c-43f1-9e41-b260c9d28947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be28d74-de56-407d-9ee7-f691402224dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "READ MODE's\n",
    "### 1️⃣ mode='permissive' (DEFAULT)\n",
    "What it does\n",
    "- Reads all records\n",
    "- Corrupt / malformed rows are not dropped\n",
    "- They are placed into a special column called _corrupt_record\n",
    "\n",
    "2️⃣ **mode='dropMalformed'**\n",
    " What it does\n",
    "- Drops malformed rows\n",
    "- No _corrupt_record column\n",
    "\n",
    "3️⃣ **mode='failFast'**\n",
    " What it does\n",
    "- Fails immediately when malformed record is found\n",
    "- Stops job execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4477b77a-2ace-4bb1-9e5e-8ae3b103b1b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "#print(rawdf1.schema)\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\", cleandf1.count())#all rows count\n",
    "display(cleandf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\",mode='dropMalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\", len(cleandf1.collect()))\n",
    "display(cleandf1)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a94e14-b7cd-4a71-a200-df677a91457c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2007342-4f59-4521-a335-093c4312451e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method3 best methodology of applying active data munging\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "struttype1 = StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "rawdf1=spark.read.schema(struttype1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\",mode='permissive')\n",
    "print(\"allow all data showing the real values\",rawdf1.count())#all rows count\n",
    "display(rawdf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7335fe-71c7-487c-8a4d-579fe3385efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Rejection Strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ccd314-7043-4a83-9e65-7d33d78ec771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Schema with corrupt record column\n",
    "strt11 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"profession\", StringType(), True),\n",
    "    StructField(\"corruptdata\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read data with PERMISSIVE mode\n",
    "dfmethod4 = spark.read \\\n",
    "    .schema(strt11) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"corruptdata\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .csv(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\")\n",
    "\n",
    "display(dfmethod4)\n",
    "\n",
    "print(\"Entire count of data:\", dfmethod4.count())\n",
    "\n",
    "# Rejected records\n",
    "df_reject = dfmethod4.filter(\"corruptdata IS NOT NULL\")\n",
    "\n",
    "# Write rejected data (excluding corrupt column)\n",
    "df_reject.drop(\"corruptdata\") \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv/\")\n",
    "print(\"Data to reject or update the source\",len(dfmethod4.where(\"corruptdata is not null\").collect()))\n",
    "display(dfmethod4.filter(\"corruptdata IS NOT NULL\")) #see rejected data\n",
    "#print df_reject.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d8dea5-89c7-4eed-8db1-62e16c421221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating rejection dataset to send to our source system for future fix\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\",mode='permissive',columnNameOfCorruptRecord=\"corruptedrows\")\n",
    "#Create a reject dataset\n",
    "rejectdf1=cleandf1.where(\"corruptedrows is not null\")\n",
    "#display(rejectdf1)\n",
    "rejectdf1.write.csv(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/reject_data/\",mode=\"overwrite\",header=True)\n",
    "retaineddf1=cleandf1.where(\"corruptedrows is null\")\n",
    "print(\"Overall rows in the source data is \",len(cleandf1.collect()))\n",
    "print(\"Rejected rows in the source data is \",len(rejectdf1.collect()))\n",
    "print(\"Clean rows in the source data is \",len(retaineddf1.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b2015a-c5f5-4eac-b869-9c82b81fbd9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before actively Cleansing or Scrubbing - We have to create a Rejection Strategy to reduce data challenges in the future\n",
    "strt11=StructType([StructField(\"id\",IntegerType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"profession\",StringType(),True),StructField(\"corruptdata\",StringType())])\n",
    "dfmethod4=spark.read.schema(strt11).csv(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\", mode=\"PERMISSIVE\",header=False,columnNameOfCorruptRecord=\"corruptdata\")\n",
    "display(dfmethod4)\n",
    "print(\"entire count of data\",dfmethod4.count())\n",
    "df_reject=dfmethod4.where(\"corruptdata is not null\")\n",
    "df_reject.drop(\"corruptdata\").write.mode(\"overwrite\").csv(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/reject_data/\",mode=\"overwrite\")\n",
    "#print(\"Data to reject or update the source\",len(dfmethod3.where(\"corruptdata is not null\").collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "802afb4a-de54-4936-8ff7-a72da5245b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cleansing**\n",
    "It is a process of cleaning/removing/deleting unwanted data\n",
    "### - **na.drop(how=\"any\")** \n",
    "- Drops the entire ROW if ANY column in that row is NULL\n",
    "- Only rows with NO nulls at all survive\n",
    "\n",
    "### - 2️⃣ **na.drop(how=\"any\", subset=[\"id\",\"age\"])**\n",
    "- Drops row ONLY IF id OR age is NULL\n",
    "- Other columns are ignored.\n",
    "\n",
    "- Mode\t              Row dropped when\n",
    "- how=\"any\"\t   ----> drop if At least one column is NULL\n",
    "- how=\"all\"\t   ----> drop if All selected columns are NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a050c9f2-94a8-48db-aae5-5eaa56dc9d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2\n",
    "#Important na functions we can use to do cleansing\n",
    "display(rawdf1.where(\"age is null\")) #raw data before cleansing\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\")#This function will drop any column in a given row with null otherwise this function returns rows with no null columns\n",
    "display(cleanseddf.where(\"age is null\"))#after cleansing no null row data will be seen\n",
    "\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\",subset=[\"id\",\"age\"]) #Drops row ONLY IF id OR age is NULL\n",
    "display(cleanseddf)\n",
    "cleanseddf=rawdf1.na.drop(how=\"all\",subset=[\"lastname\",\"profession\"])#4000004,Gretchen,,66,\n",
    "display(cleanseddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123dc8f9-89b6-4c59-887b-0d93a2d0b820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Scrubbing**\n",
    " It is a process of polishing/fine tuning/scrubbing/meaningful conversion the data in a usable format\n",
    "### -  **na.fill()** - Replaces NULL values in the specified column(s) with the given value\n",
    "### > #na.replace() is used to replace values in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01f1486-1ac3-4995-a604-1f0ff0684c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Replaces NULL values only in column lastname with 'user not provided' if NULL values are present Other columns remain unchanged.\n",
    "cleanseddf = rawdf1.na.fill('user not provided',subset=[\"lastname\"])#4000004,Gretchen,,66,\n",
    "display(cleanseddf)\n",
    "\n",
    "cleanseddf = rawdf1.na.fill('NA')#replaces all null values with 'NA'\n",
    "display(cleanseddf)\n",
    "#na.replace() is used to replace values in a DataFrame.\n",
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "find_replace_values_dict2={'not provided':'NA'}\n",
    "#Replace values in profession column #Replaces values based on dictionary mapping\n",
    "cleanseddf2=cleanseddf.na.replace(find_replace_values_dict1,subset=[\"profession\"]) #Pilot replaced with Captain\n",
    "display(cleanseddf2)\n",
    "scrubbeddf3=cleanseddf2.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "display(scrubbeddf3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1782aa6-4658-4e3d-902b-e1455e7e073c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  De-duplication?\n",
    "- De-duplication is the process of removing duplicate records from a dataset.\n",
    "- 2.Two Types of De-duplication\n",
    "### >  A. Non-Priority De-duplication\n",
    "- No rule to decide which record is better.\n",
    "- You just want one copy.\n",
    "### > B. Priority-Based De-duplication\n",
    "- Business rules decide which record to keep.\n",
    "| Step                     | Technique          | Type           |\n",
    "| ------------------------ | ------------------ | -------------- |\n",
    "| distinct()               | Exact row match    | Row-level      |\n",
    "| dropDuplicates([\"id\"])   | No priority        | Column-level   |\n",
    "| orderBy + dropDuplicates | With business rule | Priority-based |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2e33cf-44b9-4bfd-a090-c5457d1eb01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Row-level (full row) de-duplication\n",
    "display(scrubbeddf3.where(\"id in ('400001')\"))  # before row level dedup\n",
    "dedupdf1 = scrubbeddf3.distinct() #Removes rows that are 100% identical across all columns #Row-level/Non-priority de-duplication\n",
    "display(dedupdf1.where(\"id in ('4000001')\")) #Verify after row-level de-duplication\n",
    "\n",
    "#2.Column-level de-duplication (Non-priority)\n",
    "print(\"non prioritized deduplication, just remove the duplicates retaining only the first row\")\n",
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "#coalesce() is used to reduce the number of partitions in a Spark DataFrame or RDD.\n",
    "#dropDuplicates() is used to remove duplicate rows from a Spark DataFrame\n",
    "dedupdf2=dedupdf1.coalesce(1).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe) #Only one row per id\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))\n",
    "\n",
    "#3.Priority-based de-duplication logic\n",
    "print(\"prioritized deduplication based on age\")\n",
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "dedupdf1.coalesce(1).where(\"id in ('4000003')\").orderBy([\"id\",\"age\"],ascending=[True,False]).show(3)\n",
    "#manually controlling which row becomes first by: ->Sorting (orderBy) ->Then dropping duplicates\n",
    "dedupdf2=dedupdf1.coalesce(1).orderBy([\"id\",\"age\"],ascending=[True,False]).dropDuplicates(subset=[\"id\"]) #It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe72c48-c24d-4bd1-b5ad-3798dc5652c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Standardization and Replacement / Deletion of Data to make it in a usable format\n",
    "- Making the data more standard by adding/removing/reordering columns as per the expected standard, unifying into expected format, converting the type as expected etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8973888f-a02f-473e-9933-9f03f601a348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Standardization1 - Column Enrichment (Addition of columns)\n",
    "- withColumn() is used to add a new column or replace an existing column in a DataFrame\n",
    "> - df.withColumn(\"country\", lit(\"IND\"))\n",
    "- lit() creates a literal (constant) value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9e9bcf-2c9c-406d-b351-f536882ecb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap,col\n",
    "#withColumn(\"stringcolumnname to add in the df\",lit('hardcoded')/initcap(col(\"colname\")))\n",
    "standarddf1=dedupdf2.withColumn(\"sourcesystem\",lit(\"Retail\"))#SparkSQL - DSL(FBP)\n",
    "display(standarddf1.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7881a0a2-f6fd-417a-a47d-c1da4de51d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Standardization2 - Column Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de97517-4161-403c-bf8a-1c9bfa872e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "#Basic Exploration/analysis of the profession column for identifying uniformity challenges\n",
    "#standarddf1.createOrReplaceTempView(\"sqlview\")\n",
    "#display(spark.sql(\"select profession,count(*) from sqlview group by profession order by profession\"))#SQL\n",
    "#display(standarddf1.groupBy(\"profession\").count())#DSL\n",
    "#Standardization2 - column uniformity\n",
    "#------initcap()  data scientist → Data Scientist--------------\n",
    "standarddf2=standarddf1.withColumn(\"profession\",initcap(\"profession\"))#inicap or any other string function with columnOr name can accept either column or string type provided if the string is a column name for eg. profession/age/sourcesystem.\n",
    "display(standarddf2.limit(20))\n",
    "#display(standarddf2.groupBy(\"profession\").count())#DSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0533db74-5aeb-4798-840f-cf6d77cc7e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8999af4e-2676-40dc-88b3-42884e3624d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Did analysis to understand the format issues in our id and age columns\n",
    "#standarddf2.where(\"id like 't%'\").show() -------Finds rows where id contains letters ---> [a-zA-Z] → matches any alphabet\n",
    "standarddf2.where(\"id rlike '[a-zA-Z]'\").show()#rlike is regular expression like function that help us identify any string data in our DF column #[^0-9] → any character(a-z) that is NOT a digit  #[0-9] → any digit   # ^ → NOT\n",
    "standarddf2.where(\"age rlike '[^0-9]'\").show()#checking for any non number values in age column\n",
    "#standarddf3=standarddf2.withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9095eba-d9d8-4996-b7c7-b414be4e5290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,replace\n",
    "#Let's apply scrubbing features to our id column to replace ten with 10 (or we can think of using GenAI here) ---Convert words → digits\n",
    "replaceval={'one':'1','two':'2','three':'3','four':'4','five':'5','six':'6','seven':'7','eight':'8','nine':'9','ten':'10'}\n",
    "standarddf3=standarddf2.na.replace(replaceval,[\"id\"])\n",
    "#standarddf3=standarddf2.withColumn(\"id\",replace(\"id\",lit('ten'),\"10\")) \n",
    "#3-0 -> 30 #regexp_replace() → pattern based\n",
    "standarddf3=standarddf3.withColumn(\"age\",regexp_replace(\"age\",'-',\"\"))\n",
    "display(standarddf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a69c3a-3d41-4f9d-a19d-36f02d37bb12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Standardization4 - Data Type Standardization\n",
    "---standarddf3.withColumn(\"id\", **col**(\"id\").**cast**(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2adf586-f4ce-4129-a3ca-d55f35848f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf3.printSchema()#Still id and age are string type, though it contains int data\n",
    "#standarddf4=standarddf3.withColumn(\"id\",\"id\".cast(\"long\"))#this wil not work\n",
    "standarddf4=standarddf3.withColumn(\"id\",standarddf3.id.cast(\"long\"))\n",
    "standarddf4=standarddf3.withColumn(\"id\",standarddf3[\"id\"].cast(\"long\"))\n",
    "standarddf4=standarddf3.withColumn(\"id\",col(\"id\").cast(\"long\")) #best to use\n",
    "standarddf4=standarddf4.withColumn(\"age\",col(\"age\").cast(\"short\"))\n",
    "standarddf4.printSchema()\n",
    "display(standarddf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240cfc5b-f39c-4244-b514-14f4a9a721a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardization5 - Naming Standardization\n",
    "- withColumnRenamed(oldName, newName) renames ONE column at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db5b2cd7-d840-4eea-b6f6-2cec357f1474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf5=standarddf4.withColumnRenamed(\"id\",\"custid\")\n",
    "standarddf5=standarddf4.withColumnsRenamed({\"id\":\"custid\",\"sourcesystem\":\"srcsystem\"})\n",
    "display(standarddf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f128de7-32ae-4d34-a07b-2de5b7afde90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6a3ca0-96c9-4b85-904a-e121c9a3e683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf6=standarddf5.select(\"custid\", \"age\", \"firstname\",\"lastname\",\"profession\",\"srcsystem\")\n",
    "#display(standarddf6)\n",
    "mungeddf=standarddf6\n",
    "display(mungeddf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2f4f52-15ef-4b22-b306-b5746844d381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting Data Enrichment or before sharing the data to the consumer, we have to do EDA/Exploration/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fdf702c-565e-428e-b153-1e3802f32302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mungeddf.printSchema()\n",
    "display(mungeddf.take(20))\n",
    "display(\"total rows\",len(mungeddf.collect()))\n",
    "display(mungeddf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbc3049-39ef-4113-8a48-fdce007caeff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Enrichment - Detailing of data\n",
    "- Makes your data rich and detailed\n",
    "- a. Add (withColumn,select,selectExpr), Derive (withColumn,select,selectExpr), Remove(drop,select,selectExpr), Rename (withColumnRenamed,select,selectExpr), Modify/replace (withColumn,select,selectExpr) - very important spark sql functions\n",
    "- b. split, merge/Concat\n",
    "- c. Type Casting, reformat & Schema Migration\n",
    "- Data enrichment ensures data is business-ready, analytics-ready, and schema-compliant before it reaches downstream consumers.\n",
    "\n",
    "![](path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97072834-d169-4545-95ae-73352537b509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/sachinchitte4@gmail.com/Pratice/stage2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabf29f3-553c-4e6a-ac93-78898f90c5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**a. Add (), Derive (), Rename (), Modify/replace (), Remove/Eliminate () - very important spark sql DF functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc7a5d9-95bb-48d3-8cbc-77fa0b7e0332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Adding of columns\n",
    "Lets add datadt (date of the data orgniated from the source for eg. provided in the filename in a format of yy/dd/MM) and loaddt (date when we are loading the data into our system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42668e2a-349f-4d2c-8a56-59b600a0e1f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_datadt='25/30/12'\n",
    "print(f\"hello '{derived_datadt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43106b5-3d12-4218-a569-32296c035f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,current_date#already imported, not needed here\n",
    "original_filename = 'custsmodified_25/30/12.csv'\n",
    "derived_datadt = original_filename.split('_')[1].split('.')[0]\n",
    "print(f\"hello '{derived_datadt}'\") #derived_datadt='25/30/12'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "enrichdf1=mungeddf.withColumn(\"datadt\",lit('25/30/12')).withColumn(\"loaddt\",current_date()) #recommended way\n",
    "enrichdf1.printSchema() #lit('25/30/12') is a string literal  #current_date() returns a date type\n",
    "#or\n",
    "enrichdf1=mungeddf.withColumns({\"datadt\":lit('25/30/12'),\"loaddt\":current_date()})\n",
    "enrichdf1.printSchema()\n",
    "#or\n",
    "enrichdf1=mungeddf.select(\"*\",lit(derived_datadt).alias('datadt'),current_date().alias('loaddt'))#DSLs (FBP function)\n",
    "#or\n",
    "enrichdf1=mungeddf.selectExpr(\"*\",\"'25/30/12' as datadt\",\"current_date() as loaddt\")#DSL(select) + SQL expression\n",
    "enrichdf1=mungeddf.selectExpr(\"*\",f\"'{derived_datadt}' as datadt\",\"current_date() as loaddt\")#DSL(select) + SQL expression\n",
    "enrichdf1.printSchema()\n",
    "display(enrichdf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a94739c-132a-4375-9857-de2b8f9011b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Deriving of columns**\n",
    "- Engineer → E\n",
    "- Doctor   → D\n",
    "\n",
    "**- substring(colName, pos, len)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a463f0b9-f4fb-47c4-82a8-ec4f44dfd64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "enrichdf2=enrichdf1.withColumn(\"professionflag\",substring(\"profession\",1,1)) #recomendded\n",
    "#or\n",
    "enrichdf2=enrichdf1.select(\"*\",substring(\"profession\",1,1).alias(\"professionflag\"))\n",
    "#or\n",
    "enrichdf2=enrichdf1.selectExpr(\"*\",\"substr(profession,1,1) as professionflag\")\n",
    "display(enrichdf2.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af7d1f94-885f-4ec8-916a-814da6d5b0c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Renaming of columns**\n",
    "- **withColumnRenamed(oldName, newName)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d70fe01-f4dc-47df-805b-b91a352cec2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Can we use withColumn to rename? not directly, its costly\n",
    "enrichdf3=enrichdf2.withColumn(\"sourcename\",col(\"srcsystem\"))\n",
    "enrichdf3=enrichdf3.drop(\"srcsystem\").select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"professionflag\")\n",
    "#or\n",
    "enrichdf3=enrichdf2.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",col(\"srcsystem\").alias(\"sourcename\"),\"datadt\",\"loaddt\",\"professionflag\")#costly too, since we have to choose all columns in the select\n",
    "#or\n",
    "#enrichdf2.printSchema()\n",
    "enrichdf3=enrichdf2.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"srcsystem as sourcename\",\"datadt\",\"loaddt\",\"professionflag\")#costly too, since we have to choose all columns in the select\n",
    "#or\n",
    "enrichdf3=enrichdf2.withColumnRenamed(\"srcsystem\",\"sourcename\")#Best function to rename the column(s)\n",
    "#or\n",
    "enrichdf3=enrichdf2.withColumnsRenamed({\"srcsystem\":\"sourcename\",\"professionflag\":\"profflag\"})\n",
    "display(enrichdf3.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf73b8b5-d27f-4b46-8062-46cff6dac889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Modify/replace (withColumn, select/selectExpr)\n",
    "- withColumn() is used to ADD, REPLACE, or MODIFY columns.\n",
    "- withColumn() is used to add or overwrite columns. If the column exists, Spark replaces it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001bae3e-22a5-40dc-9c4e-11b197da4765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrichdf4=enrichdf3.withColumn(\"profession\",col(\"sourcename\"))#This will replace the profession with sourcename\n",
    "#or\n",
    "enrichdf4=enrichdf3.withColumn(\"profession\",concat(\"profession\",lit('-'),\"profflag\"))#This will modify/enrich the profession column with sourcename\n",
    "#or using select/selectExpr\n",
    "enrichdf4=enrichdf3.select(\"custid\",\"age\",\"firstname\",\"lastname\",concat(\"profession\",lit('-'),\"profflag\").alias(\"profession\"),\"sourcename\",\"datadt\",\"loaddt\",\"profflag\")\n",
    "#or use selectExpr\n",
    "enrichdf4=enrichdf3.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"concat(profession,'-',profflag) as profession\",\"sourcename\",\"datadt\",\"loaddt\",\"profflag\")\n",
    "display(enrichdf4.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5fbff8-555c-4563-9cf9-fb5e6b27ab71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Remove/Eliminate (drop,select,selectExpr)\n",
    "- 1️⃣ drop()  ❌ Remove columns directly (simplest)\n",
    "- 2️⃣ select() ✅ Keep only required columns\n",
    "- 3️⃣ selectExpr() – 🧠 SQL-style elimination + transformation\n",
    "\n",
    "1️⃣ drop() — Column Elimination Only\n",
    "- 📌 What it really does\n",
    "- Removes specified columns from the logical plan\n",
    "- Internally Spark applies column pruning\n",
    "- No transformation, no renaming, no expressions\n",
    "\n",
    "2️⃣ select() — Explicit Projection (Most Important)\n",
    "- 📌 What it really does\n",
    "- Defines exactly which columns Spark must read\n",
    "- Enables predicate & projection pushdown\n",
    "- Preferred by Spark optimizer (Catalyst)\n",
    "\n",
    " 3️⃣ selectExpr() — SQL Projection Layer\n",
    "- 📌 What it really does\n",
    "- Parses SQL expressions into Spark logical plan\n",
    "- Runs after SQL parser, before Catalyst\n",
    "- Same execution as select() (performance equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7826773-a59e-4e04-b330-4d52a0d31db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#enrichdf4=enrichdf3.withColumn(\"profession\",col(\"sourcename\"))#Cannot be used\n",
    "#or using select/selectExpr (yes, but costly)\n",
    "enrichdf5=enrichdf4.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\")\n",
    "#or use selectExpr (yes, but costly)\n",
    "enrichdf5=enrichdf4.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\")\n",
    "#or \n",
    "enrichdf5=enrichdf4.drop(\"profflag\")#right function to use from dropping #Basic drop() –Remove one column\n",
    "display(enrichdf5.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9130a1-e134-43cf-b684-a90effdd02d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#how to write a python program to append a variable value to another variable and use it inside the selectExpr\n",
    "name='irfan'\n",
    "sqlexpression=f\"'{name}' as owner\"\n",
    "print(sqlexpression)\n",
    "mungeddf.selectExpr(\"*\",sqlexpression).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c720fdb-a92d-414e-993f-34fb89f32e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Conclusion/Best practices of using different column enrichment functions\n",
    "1. **select** is good to use if we want to perform - \n",
    "Good for ordering/reordering of columns, only renaming column (not good), only reformatting/deriving a column (not good), **for all of these operation in a single iteration** such renaming, reordering, reformatting,deriving, dropping etc., (best to use)\n",
    "2. **selectExpr** is good to use if we want to perform - Same as select by using ISO/ANSI SQL functionality (if we are not familiar in DSL FBP) **for all of these operation in a single iteration**\n",
    "3. **withColumn** is good to use if we want to perform - \n",
    "**for adding/deriving/modifying/replacing in a single iteration**\n",
    "Adding/Deriving column(s) in the last (Good), Modifying/replacing (Good), Renaming (not good), Dropping(not possible), reordering(not good)\n",
    "4. **withColumnRenamed** is good to use if we want to perform - only for renaming column (Good)\n",
    "5. **drop** is good to use if we want to perform - only dropping of columns in the given position (Good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a252a65-2a10-4362-93f7-5cf06057f28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "b. Splitting & Merging/Melting of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532c5d5c-d1e0-4fa7-90ad-8d784ccdb471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting of column\n",
    "splitdf=enrichdf5.withColumn(\"profflag\",split(\"profession\",'-'))\n",
    "splitdf=splitdf.withColumn(\"profession\",col(\"profflag\")[0])\n",
    "#splitdf=splitdf.withColumn(\"profflag\",col(\"profflag\")[1])\n",
    "#or\n",
    "splitdf=splitdf.withColumn(\"shortprof\",upper(substring(col(\"profession\"),1,3))).drop(\"profflag\")\n",
    "#Merging of column\n",
    "mergeddf=splitdf.select(col(\"custid\"),\"age\",concat_ws(\" \",col(\"firstname\"),col(\"lastname\")).alias(\"fullname\"),\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"shortprof\")#usage of select will help us avoid chaining of withColumn,drop,select\n",
    "display(mergeddf.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f159ade1-04c2-40c2-b5ff-8b3a55891c5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**B. Splitting & Merging/Melting of Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "952d56bd-bbc0-41e8-8a9a-bc2911c29207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting of column\n",
    "splitdf=enrichdf5.withColumn(\"profflag\",split(\"profession\",'-'))\n",
    "splitdf=splitdf.withColumn(\"profession\",col(\"profflag\")[0])\n",
    "#splitdf=splitdf.withColumn(\"profflag\",col(\"profflag\")[1])\n",
    "#or\n",
    "splitdf=splitdf.withColumn(\"shortprof\",upper(substring(col(\"profession\"),1,3))).drop(\"profflag\")\n",
    "#Merging of column\n",
    "mergeddf=splitdf.select(col(\"custid\"),\"age\",concat_ws(\" \",col(\"firstname\"),col(\"lastname\")).alias(\"fullname\"),\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"shortprof\")#usage of select will help us avoid chaining of withColumn,drop,select\n",
    "display(mergeddf.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5965728b-42ee-483f-84a6-e03a353981a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergeddf.printSchema()\n",
    "#unconsious and incompetant(day1)\n",
    "#consious and incompetant(month3)-current state\n",
    "#consious and competant(further month)-another few month state\n",
    "#unconsious and competant(further month)-near end of every stage (pyspark+databricks(sql/python/dwh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e83336-74fe-428d-b76a-838ed8085e61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**c. Formatting & Typecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965953d0-8c10-4a09-b6cd-90dceb7ec208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1️⃣ Schema Evolution\n",
    "- Schema Evolution is the ability of a system to handle changes in data structure over time as new data arrives with additional or modified columns, without breaking existing pipelines.\n",
    "### Scope\n",
    "- Time-based (day over day / week over week)\n",
    "- Filesystem / table level\n",
    "- _Typical Scenario_\n",
    "- Week 1 file: id, name\n",
    "- Week 2 file: id, name, email\n",
    "- Week 3 file: id, name, email, phone\n",
    "### How it is handled\n",
    "- Use schema-aware formats (Parquet / ORC)\n",
    "- Enable mergeSchema = true at read time\n",
    "- _spark.read.option(\"mergeSchema\", \"true\").parquet(\"/data/customers\")_\n",
    "### What Spark does\n",
    "- Reads schema from all files\n",
    "- Merges them into a superset\n",
    "- Missing columns → NULL\n",
    "- Key Point\n",
    "- Schema Evolution happens because data changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ba6431-0dbc-4838-bdec-cb20c774cec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "formatteddf=mergeddf.withColumn(\"datadt\",to_date(col(\"datadt\"),'yy/dd/MM'))#25/30/12 -> 2025-12-30\n",
    "formatteddf.printSchema()\n",
    "display(formatteddf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0be6f63-a229-46b5-8857-982c573df75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Data Customization - Application of Tailored Business specific Rules\n",
    "- a. User Defined Functions\n",
    "   -  A UDF allows you to write custom logic that Spark does not provide out-of-the-box. \n",
    "- b. Building of Frameworks & Reusable Functions (We will learn very next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d54723-7c4b-4ed6-88f9-f5fe5a818f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/sachinchitte4@gmail.com/Pratice/stage3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8022f02-d589-4209-93a3-23173dedb82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#formatteddf2=formatteddf.withColumn(\"sourcename\",upper(\"sourcename\"))\n",
    "#formatteddf2.show(2)\n",
    "#Caveat - If there is no upper() function is available already in spark dsl/sql, we can either search for some functions in the online opensource platform or we have to create one (custom functions)\n",
    "#from org.apache.sql.functions import upperodd\n",
    "\n",
    "def upperodd(colname_containsvalue):\n",
    "    convertedcolvalue=colname_containsvalue.upper()\n",
    "    return convertedcolvalue\n",
    "print(upperodd(\"irfan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d8657f-c20c-4874-9758-1dbfb1a60ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "formatteddf2=rawdf1.withColumn(\"firstname\",upper(col(\"firstname\")))#we can't run python function as it is\n",
    "formatteddf2.explain()\n",
    "#display(formatteddf2.take(10))#prefer\n",
    "from pyspark.sql.functions import udf\n",
    "udfupper=udf(upperodd)#promote normal python function to spark ready udf\n",
    "formatteddf2=rawdf1.withColumn(\"firstname\",udfupper(col(\"firstname\")))#if udf is inevitable, then we create despite of performance bottleneck\n",
    "formatteddf2.explain()\n",
    "#display(formatteddf2.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b7d312-0e89-45d5-8b70-bfee55979e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create Python Custom Function with complex logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1a9418-7766-45c6-8981-103083eb5f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculating age category from the given age of the customer\n",
    "def pythonAgeCat(dfcol):\n",
    "    if dfcol is None:\n",
    "        return \"Unknown\"\n",
    "    elif dfcol<=10:\n",
    "        return \"child\"\n",
    "    elif dfcol>10 and dfcol<=18:\n",
    "        return \"teenager\"\n",
    "    elif dfcol>18 and dfcol<=30:\n",
    "        return \"young\"\n",
    "    elif dfcol>30 and dfcol<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dac5eb7-5cff-418c-ba0b-f091a1d300df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import udf library, Convert to UDF, Apply in the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eecc4464-ce1e-4dbb-9b93-e9e55dc3a084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "sparkudfageCat=udf(pythonAgeCat)\n",
    "customdf=formatteddf.withColumn(\"agecat\",sparkudfageCat(\"age\"))\n",
    "display(customdf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b413616-9837-4d5c-b9ff-ed7e761122ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 4. Data Curation/Processing (Pre Wrangling Stage) - Applying different levels of business logics, transformation, filtering, grouping, aggregation and limits applying different transformation functions\n",
    "- Select, Filter\n",
    "- Derive flags & Columns\n",
    "- Format\n",
    "- Group & Aggregate\n",
    "- Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74087fe9-ff8d-417f-a4e2-f5cb683f775f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/sachinchitte4@gmail.com/Pratice/stage3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2997a5-098f-41c3-a282-f2260486c26b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- 1.**Select, Filter**\n",
    "- In terms of Performance Optimzation - I ensured to do Push Down Optimization by doing select(project) & Filter(predicate) of what ever the expected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d6da4fc-60d3-4a1f-adca-62023bb42398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select\n",
    "#select, functions, case, literal ,from,where,group by, having, order by, limit...\n",
    "#Select few columns by filtering few rows\n",
    "selectdf=customdf.select(\"custid\",\"age\",\"agecat\",col(\"profession\").alias(\"prof\"),\"agecat\")#DSL Select\n",
    "selectdf.show(5)\n",
    "selectdf=customdf.selectExpr(\"custid\",\"age\",\"agecat\",\"profession as prof\",\"agecat\")#SQL Select\n",
    "selectdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad0cab1-27ea-42c2-9c38-81e81156276e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Filter/Where - both are literally same (filter will be used by FBP developers & where will be used by SQL developers)\n",
    "filterdf=selectdf.filter((col(\"age\")>40) & (col(\"age\")<=60))#DSL operation\n",
    "filterdf.show(5)\n",
    "filterdf=selectdf.where((col(\"age\")>40) & (col(\"age\")<=60))#DSL operation\n",
    "filterdf.show(5)\n",
    "\n",
    "filterdf=selectdf.filter(\"age>40 and age<=60\")#SQL where operation\n",
    "filterdf.show(5)\n",
    "filterdf=selectdf.where(\"age>40 and age<=60\")#SQL where operation\n",
    "filterdf.show(5)\n",
    "#filterdf.write.saveAsTable(\"filtercust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c2c504-87b8-4717-8a5d-3fb659504cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Derive flags & Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71cb43b9-6695-497e-a46e-32db8dff2aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We have created agecat using UDF (which is supposed to use only if it is inevitable)\n",
    "#But we can do the same using DSL When.otherwise or SQL CASE WHEN\n",
    "#Deriving Flag\n",
    "#Syntax in DSL: when(conditions,\"value\").when(conditions,\"value2\").otherwise(\"valuen\").alis(\"colname\")\n",
    "curateddf=customdf.select(\"*\",when(col(\"age\").isNull(),\"U\").\n",
    "                          when(col(\"age\")<=10,\"C\").\n",
    "                          when((col(\"age\")>10) & (col(\"age\")<=18),\"T\").\n",
    "                          when((col(\"age\")>18) & (col(\"age\")<=30),\"Y\").\n",
    "                          when((col(\"age\")>30) & (col(\"age\")<=50),\"M\").\n",
    "                          otherwise(\"S\").alias(\"agecatflag\"))#Suggessted than using UDFs\n",
    "display(curateddf.take(10))\n",
    "#Deriving Column\n",
    "#Syntax in SQL: case when conditions then value when conditions then value2 else valuen end as colname\n",
    "curateddf=curateddf.drop(\"agecat\").selectExpr(\n",
    "    \"*\",\"\"\"case when age is null then 'Unknown' \n",
    "                              when age<=10 then 'child' \n",
    "                              when age>10 and age<=19 then 'teenager' \n",
    "                              when age>19 and age<=30 then 'young'\n",
    "                              when age>30 and age<=50 then 'middleaged'\n",
    "                              else 'oldaged' end as agecat\"\"\")#Suggessted than using UDFs\n",
    "display(curateddf.take(10))\n",
    "#drop(\"agecat\")-> Removes the existing agecat column (if present)\n",
    "#Interview Answer of how you optimized the existing spark code developed by your ex team members?\n",
    "#I analysed the existing udfs used in my project and seeked for opurtunities to convert them into SQL/dsl based programs by implementing the udf logics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246232fc-20ba-446b-b599-1a280c57d379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''def pythonAgeCat(dfcol):\n",
    "    if dfcol is None:\n",
    "        return \"Unknown\"\n",
    "    elif dfcol<=10:\n",
    "        return \"child\"\n",
    "    elif dfcol>10 and dfcol<=18:\n",
    "        return \"teenager\"\n",
    "    elif dfcol>18 and dfcol<=30:\n",
    "        return \"young\"\n",
    "    elif dfcol>30 and dfcol<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf7f9ef-63c9-472b-a23c-ac21bb0c119a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I optimized the Spark code by identifying Python UDFs and replacing them with Spark SQL CASE WHEN and DSL when/otherwise. This removed row-level Python execution, enabled Catalyst optimization, reduced execution time, and improved scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87f61c2-ef8f-42df-8e88-5fcf16994ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.Format (Deriving Columns with different format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9c2824-6508-4b4a-b0d3-44beac3c5c11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We can use different functions - string or number or date function for format modeling\n",
    "curateddf3=curateddf.select(\"*\",datediff(\"loaddt\",\"datadt\").alias(\"delaydays\"),\n",
    "                            year(\"datadt\").alias(\"datayear\")\n",
    "                            ,month(\"datadt\").alias(\"datamonth\")\n",
    "                            ,last_day(\"datadt\").alias(\"datalastday\")).withColumn(\"agecat\",initcap(\"agecat\"))\n",
    "display(curateddf3.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3271fd65-0859-40c1-b17d-37ff36dc9131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Group & Aggregate\n",
    "- Before performing grouping or aggr, consider the below factors from the dataset....\n",
    "- identifier? cid (high in cardinality/difference) (surrogate/naturalkey)\n",
    "- descriptive? name\n",
    "- metric? avg(age),count(distinct cid),max(age),min(age)\n",
    "- measure? age,cid\n",
    "- grouping? age,prof - low in cardinality/difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d59cc3-c695-4cd6-ad82-ed7f52a8bb86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions.aggregate import avg,count,initcap,last_day,datediff,year,month,agg\n",
    "#What is the total number of customers we have?\n",
    "print(curateddf3.count())\n",
    "#What is the total number of customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\").count()\n",
    "display(curateddf4.take(100))\n",
    "#Multiple Aggregation with one grouping - What is the total number of customers,average age of those customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\").avg(\"age\").withColumnRenamed(\"avg(age)\",\"avgage\")\n",
    "display(curateddf4.take(100))\n",
    "#To calculate multiple aggregation, we need to use a function called agg function\n",
    "curateddf4=curateddf3.groupBy(\"profession\").agg(count(\"custid\").alias(\"custcount\"),avg(\"age\").alias(\"avgage\"))\n",
    "display(curateddf4.take(100))\n",
    "#curateddf4 this dataframe we materialize/store in some tables/files later\n",
    "#Multiple Aggregation with multiple grouping - What is the total number of customers,average age of those customers we have in each profession?\n",
    "curateddf4=curateddf3.groupBy(\"profession\",\"agecat\").\\\n",
    "agg(count(\"custid\").alias(\"custcount\"),avg(\"age\").alias(\"avgage\"))\n",
    "display(curateddf4.take(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d007f695-19f4-48aa-851d-d905baf91b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Ordering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a0f9c7-f389-41f5-8f5f-565ed5c11430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curateddf5=curateddf4.orderBy(\"profession\",\"agecat\")\n",
    "display(curateddf5.take(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2731d173-9b48-4fdc-a371-1c0ac674ee97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6. Limit**\n",
    "\n",
    "- Let us take an oppurtunity to understand different data limiting/restricting functions\n",
    "- Limit is a dataframe TRANSFORMATION functions used to limit the number of rows returned in a spark dataframe FORMAT\n",
    "- Take is a dataframe/RDD ACTION functions used to limit the number of rows returned in a python list FORMAT\n",
    "- display is a standard output databricks specific function used to produce entire DF/List output in a notebook view with multiple options\n",
    "- show is a standard output spark dataframe specific function used to produce default 20 rows of a DF output in a notebook/REPL/IDE view\n",
    "- collect is a spark action that help us collect the DF/RDD data into the driver environment in a form of python list\n",
    "| Function    | Type           | Returns        | Executes? | Safe for big data |\n",
    "| ----------- | -------------- | -------------- | --------- | ----------------- |\n",
    "| `limit()`   | Transformation | DataFrame      | ❌ No      | ✅ Yes             |\n",
    "| `take()`    | Action         | Python list    | ✅ Yes     | ⚠ Small only      |\n",
    "| `display()` | Output         | UI             | ✅ Yes     | ⚠ Limited         |\n",
    "| `show()`    | Action         | Console output | ✅ Yes     | ⚠ Limited         |\n",
    "| `collect()` | Action         | Python list    | ❌❌        | ❌ No              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b362fe-c704-4b79-a893-318a2c682ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#anything can be used under display()\n",
    "print(\"limit output\")\n",
    "curateddf5.limit(20).show(10)\n",
    "print(\"take output\")\n",
    "curateddf5.take(10)\n",
    "#When to use what\n",
    "#I have to filter some data in a limited dataset of 100 rows\n",
    "curateddf5.limit(100).filter(\"profession='Accountant'\").show()\n",
    "#Display\n",
    "display(curateddf5.limit(100).filter(\"profession='Accountant'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e70f7e1-0b8f-4511-a398-d4150a8408ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Data Wrangling - More of Analytics + Transformation**\n",
    "- 1.Joins - Relation/Connection established between one or more datasets/df/tabl to produce the broader/extended view of the data horizontally. 5 Categories of Joins: inner, outer(left right, full), self, cross, special optimized (semi, anti)\n",
    "- 2.Lookup\n",
    "- 3.Lookup & Enrichment\n",
    "- 4.Schema Modeling (Denormalization)\n",
    "- 5.Windowing\n",
    "- 6.Analytical\n",
    "- 7.Set operations\n",
    "- 8.grouping & aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280dfb4b-385e-4d06-b278-4d58688c8493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/sachinchitte4@gmail.com/Pratice/stage5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d9f919-f585-4c35-8deb-4de951c9853a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Joins**\n",
    "Joins are Relation/connection of one or more tables to perform widened (horizontal) data analytics\n",
    "- Frequently used simple joins (inner, left)\n",
    "- _df_left.join(df_right,  on=\"custid\",  how=\"inner\")_\n",
    "- InFrequent simple joins (self, right, full, cartesian)\n",
    "- Advanced joins (Semi and Anti)\n",
    "- Syntax - dfleft.join(dfright,how='typeofjoin',on='custid'=='cid')\n",
    "- Optimized joins (Broadcast join, SMB Join, Shuffle hash join, Map/Reduceside join, Skewed join etc.)-AQE (Adaptive Query Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef138e4-2eb1-461a-812e-c0fb7ebe4030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ques1: In what order i can apply the functions in dataframe curation/transformation\n",
    "#Although Spark’s Catalyst optimizer may reorder operations, I apply selective filters before expensive operations like na.drop() to reduce data volume early and improve performance\n",
    "rawdf1=rawdf1.na.drop().where(\"id<>'ten' and id<>'trailer_data:end of file'\")\n",
    "rawdf1.explain()\n",
    "rawdf1=rawdf1.where(\"id<>'ten' and id<>'trailer_data:end of file'\").na.drop()\n",
    "rawdf1.explain()\n",
    "#Ques2:If I wanted to join more than 1 tables\n",
    "rawdf2=rawdf1.withColumn(\"multiconcat\",concat(concat(lit('a'),lit('b'))))\n",
    "rawdf3=rawdf2\n",
    "joineddf1=rawdf1.join(rawdf2,how='inner')\n",
    "rawdf4=rawdf2\n",
    "joineddf1.join(rawdf3,how='left').join(rawdf4,how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de8dcef-62c0-4f50-a57d-3b3baa1e054a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Correct order for most pipelines\n",
    "- where() / filter() → row reduction\n",
    "- select() → column pruning\n",
    "- na.drop() / fill() → cleanup\n",
    "- withColumn() → derivations\n",
    "- groupBy() / join() → heavy ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "038e0f80-4a09-4ae7-ac01-2d601f98c4e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,col\n",
    "#How to write join syntax in Spark and learn the semantics of join in spark\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "\n",
    "rawdf1=spark.read.csv(\"/Volumes/workspace/default/usage_metrics/custsmodified\",header=False,inferSchema=False).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1=rawdf1.na.drop().where(\"id<>'ten' and id<>'trailer_data:end of file'\")\n",
    "rawdf1=rawdf1.where(\"id<>'ten' and id<>'trailer_data:end of file'\").na.drop()\n",
    "leftdf=rawdf1.select(\"id\",\"age\",\"firstname\",\"lastname\").where(\"id in (4000100,4000101)\")\n",
    "rightdf=rawdf1.select(\"id\",\"profession\").where(\"id in (4000100,4000102,4000103)\")\n",
    "leftdf.show(20,False)\n",
    "rightdf.show(20,False)\n",
    "#Let's understand all types of joins syntax & semantics quickly\n",
    "#dfleft.join(dfright,how='typeofjoin',on='custid'=='cid')\n",
    "\n",
    "#5 Categories of Joins : 1. inner(important), 2. outer(left(important), right, full), 3. special optimized(important) (semi, anti), 4. self, 5. cross\n",
    "#1. inner, 2. outer(left, right, full)\n",
    "#1. inner\n",
    "#Usecase - Ensure which ever the customers (from both df) matches - Functionality(application)\n",
    "print(\"inner\")\n",
    "innerjoindf=leftdf.join(rightdf,how='inner',on='id')\n",
    "innerjoindf.show(20)#only 4000100 from both df is displayed\n",
    "\n",
    "#2. outer(left, right, full)\n",
    "#2. left\n",
    "#Usecase - Ensure all of the master customers with/without profession provided - Functionality(application)\n",
    "print(\"left\")\n",
    "leftjoindf=leftdf.join(rightdf,how='left',on='id')#syntax(how to do)\n",
    "leftjoindf.show(20)#semantics(what is the output) - only 4000100,4000101 from both df is displayed with non applicable nulls in the right\n",
    "\n",
    "#2. right\n",
    "#Usecase - Ensure all of the customers with professions has to be displayed\n",
    "print(\"left\")\n",
    "rightjoindf=leftdf.join(rightdf,how='right',on='id')#syntax(how to do)\n",
    "rightjoindf.show(20)#semantics(what is the output) - only 4000100,4000102,4000103 from both df is displayed with non applicable nulls in the left\n",
    "\n",
    "#2. full\n",
    "#Usecase - Ensure all of the customers with/without master customers or professions has to be displayed\n",
    "print(\"full\")\n",
    "fulljoindf=leftdf.join(rightdf,how='full',on='id')#syntax(how to do)\n",
    "fulljoindf.show(20)#semantics(what is the output) - displays all 4000100,4000101,4000102,4000103 from both df is displayed with non applicable nulls from both left and right\n",
    "\n",
    "#3. special optimized (semi, anti)\n",
    "#Why semi/anti is an optimized join?\n",
    "#Interview answer - I found low hanging fruits oppurtunities in my project.. converting inner/left joins to semi/anti, because the joins are good in performance (it uses EXISTS / NOT EXISTS condition rather than in condition)\n",
    "#semi\n",
    "#Usecase - Ensure which ever the customers (from both df) matches - Functionality(application)\n",
    "print(\"left semi/semi\")\n",
    "semijoindf=leftdf.join(rightdf,how='leftsemi',on='id')\n",
    "semijoindf.show(20)#only 4000100 (matches between both df) from LEFT df is displayed\n",
    "\n",
    "#anti\n",
    "#Usecase - Ensure which ever the customers (from both df) matches - Functionality(application)\n",
    "print(\"left anti/anti\")\n",
    "antijoindf=leftdf.join(rightdf,how='leftanti',on='id')\n",
    "antijoindf.show(20)#only 4000101 (un matched between both df) from LEFT df is displayed\n",
    "\n",
    "#4. Self join\n",
    "#Usecase - Hierarchical Retrival or join - Data joined by itself to produce the relational output of the self dataset\n",
    "print(\"self\")\n",
    "custaffliatedf=leftdf.withColumn(\"refcustid\",lit('4000100'))\n",
    "custaffliatedf.show(3)\n",
    "selfjoindf=custaffliatedf.alias(\"l\").join(custaffliatedf.alias(\"r\"),how='inner',on=(col('l.id')==col(\"r.refcustid\")))\n",
    "\n",
    "selfjoindf.select('l.id','r.*').show(20)#only 4000101 (un matched between both df) from LEFT df is displayed\n",
    "\n",
    "#5. cross join\n",
    "#What join it will perform by default? inner join (if we use on condition), cartesian/cross join (if no on condition used)\n",
    "#Minimum syntax to use?\n",
    "#cross\n",
    "print('cartesian/cross')\n",
    "joindf=leftdf.join(rightdf)#without on it is cross/cartesian product (very costly and avoidable join)\n",
    "joindf.show(20)#6 rows returned\n",
    "\n",
    "print('inner by default')\n",
    "joindf=leftdf.join(rightdf,on='id')#default inner join\n",
    "joindf.show(20)#4000100 returned\n",
    "\n",
    "print('expecting right join, but because of lack of on, cross join happened')\n",
    "joindf=leftdf.join(rightdf,how='right')#without on it is cross/cartesian product (very costly and avoidable join)\n",
    "joindf.show(20)#6 rows returned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e7e2fd-df27-4beb-97ef-755a2297b834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I identified performance improvement opportunities by converting inner and left joins into semi and anti joins wherever the requirement was only to check existence. Semi and anti joins avoid dataset widening, reduce shuffle and memory usage, and internally use EXISTS/NOT EXISTS semantics, making them much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a67c24-dfd9-4498-9705-3284c4b5d108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Applications of Joins\n",
    "Requirement: I need to do analytics/reporting of top 3 customers who did highest amount of transactions in our business last year, so i can send them some offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd296c8-e8d2-4b66-bf22-a1fa8e564130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#We already have customer master (Qualifying/dimension) data in a curated state\n",
    "curateddf3.show(5)#This contains only customer curated data (munged, enriched, customized, curated)\n",
    "#We need to bring transaction detailed (Quantifying/Fact) data upto curated state (we need to follow all/any/none of the steps that we followed so far munged, enriched, customized, curated, wrangle etc., depends on the EDA result)\n",
    "#strt1=\"txnid long,txndt string,custid int,amt double,category string,product string,city string,state string,spendby string\"\n",
    "strt1=StructType([StructField('txnid', IntegerType(), True), StructField('txndt', StringType(), True), StructField('custid', IntegerType(), True), StructField('amt', DoubleType(), True), StructField('category', StringType(), True), StructField('product', StringType(), True), StructField('city', StringType(), True), StructField('state', StringType(), True), StructField('spendby', StringType(), True)])\n",
    "#txnsrawdf=spark.read.csv(\"/Volumes/workspace/default/volumewd36/txns_2025.txt\",inferSchema=True,header=False)\n",
    "#txnsrawdf.schema\n",
    "txnsrawdf=spark.read.schema(strt1).csv(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/txns_2025.txt\",header=False)\n",
    "txnsrawdf.printSchema()\n",
    "txnsrawdf.show(5)\n",
    "txnsrawdf.summary().show(100)\n",
    "#As per the above output, we can do some transformations of munging, enrichment etc.,\n",
    "txnsmungeddf=txnsrawdf.na.drop().dropDuplicates([\"txnid\"])\n",
    "txnsenrichdf=txnsmungeddf.withColumn(\"surrogatekey\",monotonically_increasing_id())\n",
    "\n",
    "txnscurateddf=txnsenrichdf.withColumn(\"txndt\",to_date(col(\"txndt\"),\"MM-dd-yyyy\"))\n",
    "#If value = \"2024/12/25\"\n",
    "#Format = \"MM-dd-yyyy\"\n",
    "txnscurateddf.printSchema()\n",
    "txnscurateddf.show(5)#Completed munging, enrichment and curation\n",
    "\n",
    "#Now lets achieve the solution for the business requirement:\n",
    "#Requirement: I need to do analytics/reporting of top 3 customers who did highest amount of transactions in our business last month, so i can send them some offers.\n",
    "#spark.sql(\"select month(add_months(current_date(), -1))\").show()\n",
    "txnsdecdf=txnscurateddf.where(\"month(txndt)=month(add_months(current_date(), -1))\")\n",
    "print(\"dec transactions count \",txnsdecdf.count())\n",
    "\n",
    "custdimdf=curateddf3\n",
    "joineddf=custdimdf.join(txnsdecdf,how=\"inner\",on=\"custid\")\n",
    "joineddf.show(10)\n",
    "#We have to generate a reporting table with only custid,name,profession,amount,spendby\n",
    "reportdf=joineddf.select(\"custid\",\"fullname\",\"amt\",\"spendby\").orderBy([\"amt\"],ascending=[False]).limit(3)\n",
    "reportdf.show()#Top 3 transacting customer info (in the overall data)\n",
    "reportdf=joineddf.select(\"custid\",\"fullname\",\"amt\",\"spendby\").orderBy([\"amt\"],ascending=[True]).limit(3)\n",
    "reportdf.show()#Least 3 transacting customer info (in the overall data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33eb9be2-b8ad-4851-aeaf-1ddbb41639df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Lookup (only exists/non exists check) - semi or anti**\n",
    "Lookup is the process of looking up for some data attributes using the key to identify the presence of the values (not the actual values are returned) Eg. whether this particular customer made a transaction or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65556c61-ba59-4292-b96e-c296e225457f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lookup: Show me only the customer information, who did transactions last month (i dont need what transaction)?\n",
    "#What type of best join i have to use? semi join is best and faster\n",
    "custonlylastmonthtransacteddf=custdimdf.join(txnsdecdf,how='semi',on='custid')\n",
    "custonlylastmonthtransacteddf.show(2)\n",
    "print(custonlylastmonthtransacteddf.count())\n",
    "#I store this data in a table, my business analysts will apply filter a particulatr customer\n",
    "\n",
    "#It is possible to produce the same result by using inner or left join also, but not preferred just for lookup\n",
    "custonlylastmonthtransacteddf=custdimdf.alias(\"cust\").join(txnsdecdf,how='inner',on='custid')\n",
    "resultdf=custonlylastmonthtransacteddf.select(custdimdf[\"*\"]).distinct()\n",
    "print(resultdf.count())\n",
    "\n",
    "custonlylastmonthtransacteddf=custdimdf.alias(\"cust\").join(txnsdecdf,how='left',on='custid')\n",
    "resultdf=custonlylastmonthtransacteddf.select(custdimdf[\"*\"]).where(\"txnid is not null\").distinct()\n",
    "print(resultdf.count())\n",
    "\n",
    "#Lookup: Show me only the customer information, who did not do transactions last month?\n",
    "custonlylastmonthnontransacteddf=custdimdf.join(txnsdecdf,how='anti',on='custid')\n",
    "custonlylastmonthnontransacteddf.show(2)\n",
    "print(custonlylastmonthnontransacteddf.count())\n",
    "\n",
    "custonlylastmonthtransacteddf=custdimdf.alias(\"cust\").join(txnsdecdf,how='left',on='custid')\n",
    "resultdf=custonlylastmonthtransacteddf.select(custdimdf[\"*\"]).where(\"txnid is null\").distinct()\n",
    "print(resultdf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c4dcea-d320-4423-9f32-994d4e142f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Lookup & Enrichment\n",
    "Lookup and enrichment using joins - left_join (best), inner, right/full(least bother) Lookup and enrichment is the process of looking up for for some data attributes using the key and enrich the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331a1c59-21bb-4d11-be91-c3a13f07dfa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lookup & Enrichment: Show me both the customer information and transactions information of last month (i need what are the transactions made by the given customer)?\n",
    "#What type of best join i have to use? inner (only those who did transactions) or left join (all customers)\n",
    "#left\n",
    "custonlylastmonthtransacteddf=custdimdf.join(txnsdecdf,how='left',on='custid')\n",
    "custonlylastmonthtransacteddf.show(2)\n",
    "print(custonlylastmonthtransacteddf.count())\n",
    "#inner\n",
    "custonlylastmonthtransacteddf=custdimdf.join(txnsdecdf,how='inner',on='custid')\n",
    "custonlylastmonthtransacteddf.show(2)\n",
    "print(custonlylastmonthtransacteddf.count())\n",
    "\n",
    "#We can't achieve the same result by using semi join, because it will not return the transaction info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3656c951-3b44-4d29-8cb0-5f78901fb38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3.**Schema Modeling (Denormalization-joined result of tables)-DWH/DataMart**\n",
    "- Use inner(mostly),left/right/full(we can use depends on the business) We can build Datawarehouse components (dimension, fact tables) appling joins on the tables to achieve different types of schemas\n",
    "\n",
    "- Star Schema txns_fact cust_mast_dim(dim)\n",
    "- id,amt cid,name,age,city_lived 1,100 11,irfan,44,Bangalore|Chennai\n",
    "- Snowflake Schema - txns_fact cust_mast_dim(dim) cust_det_dim(subdim) id,amt cid,name,age cid,city_lived 1,100 11,irfan,44 11,Bangalore 11,Chennai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a923da3-7a39-48d1-adb0-b324800fde85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Implementing Star schema model\n",
    "#In our current dataset, we have what schema model can be defined? Only star schema is possible, because we have only one fact table and multiple dimension tables (no sub dimension).\n",
    "#What is the best way to join all the tables? We can use inner join, because we have only one fact table and multiple dimension tables.\n",
    "#inner\n",
    "denormalized_fat_wide_df=custdimdf.join(txnsdecdf,how='inner',on='custid')\n",
    "denormalized_fat_wide_df.show(2)\n",
    "#I store this denormalized dataframe data into final fact table (in a single table) to simplify my customer queries without applying joins.\n",
    "print(denormalized_fat_wide_df.count())\n",
    "\n",
    "#or We can use left join also to ensure all customer info is captured.\n",
    "#left\n",
    "denormalized_fat_wide_df=custdimdf.join(txnsdecdf,how='left',on='custid')\n",
    "denormalized_fat_wide_df.show(2)\n",
    "print(denormalized_fat_wide_df.count())\n",
    "\n",
    "#or We can use full join also to ensure all customer (and+or) transaction info is captured.\n",
    "#left\n",
    "denormalized_fat_wide_df=custdimdf.join(txnsdecdf,how='full',on='custid')\n",
    "denormalized_fat_wide_df.show(2)\n",
    "print(denormalized_fat_wide_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09eddf2a-8936-4dd7-9a37-58f22e03a50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4.Windowing Functionalities\n",
    "- Is the concept of grouping/bucketize/dividing/partitioning and performing some analytical operation on the literally partitioned data Benifits of Windowing Functionality\n",
    "- \n",
    "- Creating Surrogate/primary key/seq number\n",
    "- Performing Top N Analysis\n",
    "- Duplicate Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df70165-4a78-40cb-93bf-ed179ad03605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "denormalized_fat_wide_df=custdimdf.join(txnsdecdf,how='inner',on='custid')\n",
    "print(denormalized_fat_wide_df.count())\n",
    "orderjoineddf=denormalized_fat_wide_df.where(\"custid in (4000022,4000816)\").select(\"custid\",\"age\",\"profession\",\"txndt\",\"amt\",\"category\",\"product\",\"city\",\"state\",\"spendby\")\n",
    "#display(orderjoineddf)\n",
    "\n",
    "#We are using row_number() window function (very important)\n",
    "#Synax for windowing function\n",
    "#from pyspark.sql.window import Window\n",
    "#select(row_number().over(Window.partitionBy(\"custid\").orderBy(\"txndt\"))).alias(\"seqnum\"))\n",
    "\n",
    "#1.Creating Surrogate/primary key/seq number\n",
    "#Let us perform a non windowing operation that means we are not using partitioning\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.orderBy(\"custid\",\"txndt\")))#overall sorting\n",
    "display(sk_orderjoinedf)\n",
    "\n",
    "#2. Performing Top N Analysis\n",
    "#Let us perform windowing operation\n",
    "#Interview Questions pattern:\n",
    "print(\"a. Tell me the top 1 transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\")))).where(\"seqnum=1\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"b. Tell me the least 1 transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(\"amt\"))).where(\"seqnum=1\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"c. Tell me the top 2 transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\")))).where(\"seqnum<=2\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"d. Tell me the just the 2nd highest transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\")))).where(\"seqnum=2\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"e. Tell me the just the 2nd highest transaction made by the customer within the given state\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\",\"state\").orderBy(desc(\"amt\")))).where(\"seqnum=2\")\n",
    "display(sk_orderjoinedf)\n",
    "\n",
    "print(\"f. Show me the very first time the business made by the given customers?\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(\"txndt\"))).where(\"seqnum=1\")\n",
    "display(sk_orderjoinedf)\n",
    "print(\"g. Show me the very first two transaction made by the given customers?\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(\"txndt\")))\\\n",
    ".where(\"seqnum<=2\")\n",
    "display(sk_orderjoinedf)\n",
    "\n",
    "#3. Duplicate Handling\n",
    "#I want to remove duplicates based on custid with same spendby (for a given customer i need only one credit and one cash transaction info)\n",
    "print(\"I want to get the duplicate custid with the same spendby removed out from our data?\")\n",
    "#display(sk_orderjoinedf.coalesce(1).dropDuplicates([\"custid\",\"spendby\"]))\n",
    "#Using windowing function we can do controlled way of dropping duplicates, for eg. i want to drop the repeating data of same customer and spendby by retaining only the latest transactions\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\",\"spendby\").orderBy(desc(\"txndt\"))))\n",
    "display(sk_orderjoinedf)\n",
    "display(sk_orderjoinedf.where(\"seqnum=1\"))\n",
    "\n",
    "#but if amt is same for 2 transaction of diff dates? - rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2194aae3-f0f2-45e7-9678-1f73a7250d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Second important window functions are rank() - used for applying same rank for same values and will have gaps (don't maintain continuity) \n",
    "# and dense_rank() - used for applying same rank for same values and will not have gaps (maintain denser/close continuity) \n",
    "print(\"a. Tell me the just the 2nd highest transaction made by the customer\")\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"rnk\",rank().over(Window.partitionBy(\"custid\").orderBy(desc(\"spendby\")))).withColumn(\"densernk\",dense_rank().over(Window.partitionBy(\"custid\").orderBy(desc(\"spendby\"))))\n",
    "display(sk_orderjoinedf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca0c07a-4d48-448c-8ae1-4c40614b0d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5.Analytical Functionalities**\n",
    "Performing analytics/summarization/categorization of data by applying/not by applying windowing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e4afb8-44a2-43f5-a810-d192285c6e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Comparitive analytics - lead & lag\n",
    "sk_orderjoinedf=orderjoineddf.withColumn(\"nexttransamt\",lead(\"amt\",1,0).over(Window.partitionBy(\"custid\").orderBy(asc(\"txndt\")))).withColumn(\"priortransamt\",lag(\"amt\",1,0).over(Window.partitionBy(\"custid\").orderBy(asc(\"txndt\"))))\n",
    "display(sk_orderjoinedf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63a07f5-cc60-4719-a0fe-17f09bd55127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](D:\\Data_Engineer_docs\\data_docs\\stage3.png)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8580293281368747,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) BreadandButter_program",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
