{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db863fa0-cd5f-49cd-a3df-a7d05b7e3729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark1= SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8815e7e2-e212-4025-9535-f384550c9fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1)Create Catalog\n",
    "2)Create Schema (Database)\n",
    "3)Create Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c896267-565e-4423-ad77-205b204705b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS telecom_catalog_assign;\n",
    "CREATE SCHEMA IF NOT EXISTS telecom_catalog_assign.landing_zone;\n",
    "CREATE VOLUME IF NOT EXISTS telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85781828-20a2-49c1-8656-7afbec802868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create folders using dbutils.fs.mkdirs\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "408bb048-e4d3-49be-9dec-89885021b09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- > a. Volume vs DBFS/FileStore\n",
    "- > _D_B_FS/FileStore is for convenience and temporary files, while Volumes are for secure, governed, production-grade data storage.___\n",
    "\n",
    "-** Volumes**\n",
    "- Governed by Unity Catalog\n",
    "- Fine-grained access control (catalog/schema/volume)\n",
    "- Full auditing and lineage\n",
    "- Secure and compliant (GDPR, HIPAA, SOC2)\n",
    "- Recommended for production systems\n",
    "\n",
    "-** DBFS / FileStore**\n",
    "- Not governed by Unity Catalog\n",
    "- Limited access control\n",
    "- Minimal auditing\n",
    "- Meant for temporary or non-regulated data\n",
    "- Not suitable for production use\n",
    "\n",
    "b. Why production teams prefer Volumes for regulated data\n",
    "- Provide strong security and access control\n",
    "- Enable auditing and compliance reporting\n",
    "- Support regulatory standards (GDPR, HIPAA, SOC2)\n",
    "- Allow environment isolation (dev / qa / prod)\n",
    "- Backed by secure cloud storage (S3, ADLS, GCS)\n",
    "- Reduce risk of data leaks and unauthorized access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d52b6d62-d755-4ce9-b8f5-4d30e6e09aa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data files to use in this usecase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7fd66b7-c5ff-4a17-bb1e-672683e50e36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "dbutils.fs.put(path, contents, overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebfab86d-1be4-4f94-b965-dacfbc066483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52061b4e-f0c6-4544-a7f4-7f6910bab814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = \"\"\"101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "\"\"\"\n",
    "\n",
    "usage_tsv = \"\"\"customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "\"\"\"\n",
    "\n",
    "tower_logs_region1 = \"\"\"event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ebe8eb7-71f4-4be0-afd6-5824ba41dbfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STEP 1: Write raw data to Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18272369-afb0-4f0c-8ae1-22131015d65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "dbutils.fs.put(path, content, overwrite=True)\n",
    "- A Databricks filesystem utility\n",
    "- Writes plain text content to storage\n",
    "- Runs on driver only\n",
    "- No parallel execution\n",
    "- No schema\n",
    "- No data validation\n",
    "- Fast for small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c90ab4-1b1d-411a-ba47-19e3d58af0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",\n",
    "    customer_csv,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "dbutils.fs.put(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",\n",
    "    usage_tsv,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "dbutils.fs.put(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_region1.txt\",\n",
    "    tower_logs_region1,\n",
    "    overwrite=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9cb35c7-4ea2-42cd-938e-f89101a10e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STEP 2: Define Manual Schemas\n",
    "- StructField(column_name, data_type, nullable)\n",
    "- True → column can contain NULL values\n",
    "- False → column must NOT contain NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b77a49-44b5-4340-831b-9faff3c0e11c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), True),\n",
    "    StructField(\"cust_name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),   # abc exists → string\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"plan_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "usage_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"voice_mins\", IntegerType(), True),\n",
    "    StructField(\"data_mb\", IntegerType(), True),\n",
    "    StructField(\"sms_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "tower_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"tower_id\", StringType(), True),\n",
    "    StructField(\"signal_strength\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fda968fc-ee94-46e7-ac68-a0e3914bb323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### STEP 3: Create DataFrames\n",
    "- This code reads a CSV file from a Databricks Volume using a manually defined schema and delimiter, treating all rows as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee39e34-e91c-417f-9016-4901e98e2280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df = spark.read\n",
    "    .schema(customer_schema) \n",
    "    .option(\"header\", \"false\") \n",
    "    .option(\"sep\", \",\") \n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "usage_df = spark.read \\\n",
    "    .schema(usage_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "    \n",
    "tower_df = spark.read \\\n",
    "    .schema(tower_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_region1.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95c2b60e-e0d6-428b-a702-b28f13979944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7dd887c0-1dd3-4129-8d53-d0ef34e6a2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df.show()\n",
    "customer_df.printSchema()\n",
    "\n",
    "usage_df.show()\n",
    "tower_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53a2077f-d6b9-4f24-93dc-ea9c391097bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da96fd4-bcb2-42ef-a966-e4270fdb7157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Directory Read Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7155fd22-0266-4586-b786-684f072b9c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Path Glob Filter (pathGlobFilter)\n",
    "✔ Reads\n",
    "- Only .csv files\n",
    "- Only from one directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde409d0-88d3-43ea-b4c2-ee7308b9ba3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Only read CSV files in region1\n",
    "tower_df_glob = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.csv\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\")\n",
    "#here it reads .csv files only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "843d8d13-11d8-49c9-92d0-ce7cb9f7c121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Multiple Paths Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b8ed38-4394-49b2-854e-a4edde7066b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_df_multi = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv([\n",
    "        \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\n",
    "        \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"\n",
    "    ])\n",
    "\n",
    "tower_df_multi.show()\n",
    "    #Reads All files from listed paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1121758d-d6f0-449f-93c6-750697970253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Recursive Lookup (recursiveFileLookup) ->  Read all files inside all subfolders automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c2ee0b-174e-44ee-9dfc-70febc73760b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_df_recursive = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "\n",
    "tower_df_recursive.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3637038-15e3-487b-a784-d2b5d7fcf470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Glob filter → pattern-based read\n",
    "\n",
    "Multiple paths → controlled read\n",
    "\n",
    "Recursive lookup → full directory scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbefaa40-ccae-4d56-8c7f-dbf00cd1613d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Case 1: header=False, inferSchema=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2f804e2-77b3-40d5-9ad1-1b76a1a70f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "samplingratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8304fdba-03b2-4bab-ae4f-59ea943d1b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df1 = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",\n",
    "    header=False,\n",
    "    inferSchema=False,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "customer_df1.show()\n",
    "customer_df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de12f3b0-05b1-43b9-8fc1-63c32eec535e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Case 2: header=True, inferSchema=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f130300c-9f64-441c-a168-1d5d859fae3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df2 = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "customer_df2.show()\n",
    "customer_df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02eae5af-31df-4afa-a480-1d8f00e642ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Option                              | Behavior                                        |\n",
    "| ----------------------------------- | ----------------------------------------------- |\n",
    "| header=False                        | Spark treats **first row as data**              |\n",
    "| header=True                         | Spark treats **first row as column names**      |\n",
    "| inferSchema=False                   | All columns are **string**                      |\n",
    "| inferSchema=True                    | Spark tries to **guess column types**           |\n",
    "| inferSchema=True + bad data (`abc`) | Column becomes **string** to handle mixed types |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad10f385-06c5-406e-bcaf-836e7ec1ce65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3️⃣ How Spark handles \"abc\" in age\n",
    "- age column has mostly integers\n",
    "- One value is \"abc\"\n",
    "- inferSchema=True → Spark cannot cast to integer\n",
    "- Result → Column type is string (to accommodate all values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd2aca04-fef4-4fb1-b1d4-1d3867a29b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "✅ Summary Notes\n",
    "Use header=True when the CSV contains column names.\n",
    "Use inferSchema=True to automatically detect types.\n",
    "If column has mixed types, Spark picks string.\n",
    "header=False + inferSchema=False → Safe fallback for raw ingestion; all data is string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50e94d64-a4bf-4ac8-9e6a-54c16e0b87e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Customer Data – Rename Columns Using toDF\n",
    "- toDF() allows renaming columns easily.\n",
    "- All columns are string by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97b78aa-b274-4c83-a611-abd9d77c0bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = \"\"\"101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "\"\"\"\n",
    "\n",
    "customer_df = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",\n",
    "    header=False,\n",
    "    inferSchema=False,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "# Rename columns using toDF(wrapper)\n",
    "customer_df = customer_df.toDF(\"cust_id\", \"cust_name\", \"age\", \"city\", \"plan_type\")\n",
    "\n",
    "customer_df.show()\n",
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce70286-a9ef-499b-8f18-0cc9b3880228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Usage Data – Apply Columns & Datatypes Using schema Function\n",
    "- Using schema ensures proper datatypes.\n",
    "- header=True uses first row as column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7a94e9-a5c6-41b8-8242-b4bf63d6b594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_tsv = \"\"\"customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Define schema for usage data\n",
    "usage_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"voice_mins\", IntegerType(), True),\n",
    "    StructField(\"data_mb\", IntegerType(), True),\n",
    "    StructField(\"sms_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Read usage TSV with schema\n",
    "usage_df = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=True,\n",
    "    schema=usage_schema\n",
    ")\n",
    "\n",
    "usage_df.show()\n",
    "usage_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32322e67-09b3-42e4-ac42-151e5a129e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "tower_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"tower_id\", StringType(), True),\n",
    "    StructField(\"signal_strength\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)  # Use TimestampType() if you want\n",
    "])\n",
    "\n",
    "tower_df = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_region1.txt\",\n",
    "    sep=\"|\",              # File is pipe-delimited\n",
    "    header=True,          # First row is header\n",
    "    schema=tower_schema   # Apply manual schema\n",
    ")\n",
    "\n",
    "tower_df.show()\n",
    "tower_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5312f447-dbfe-4fcf-a2f5-ecf4960a5755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Dataset  | Method       | Notes                               |\n",
    "| -------- | ------------ | ----------------------------------- |\n",
    "| Customer | `toDF()`     | Rename columns, types remain string |\n",
    "| Usage    | `.schema()`  | Column names + datatypes explicitly |\n",
    "| Towers   | `StructType` | Column names + detailed datatypes   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06213b6d-6656-41ae-90ad-451bc83170a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Code to Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d86924-0f38-4228-a95f-9e81aae1053d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df = spark.read \\\n",
    "    .schema(customer_schema) \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2ae0d86-51d8-41cc-afcc-93223fcabfdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark .write Operations"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5378930304523455,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Usecase3_Read_write",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
