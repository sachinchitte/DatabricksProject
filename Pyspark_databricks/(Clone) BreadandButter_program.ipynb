{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ca193f2-8bbd-46aa-9320-964abe63305d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce6b23ae-2025-4823-8e73-3b78d65e34bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data munging is the process of converting raw data into a usable format by cleaning, transforming, and enriching it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672d1231-c742-4a26-a5da-40b4bba311ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Passive Data Munging : Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns.**\n",
    "1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality\n",
    "- Null columns are there\n",
    "- duplicate rows\n",
    "- format issues are there (age is not in number format eg. 7-7)\n",
    "- Uniformity issues (Artist, artist)\n",
    "- Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b32287-64b6-4cca-8ff1-874d91e8dd36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbbb281-d1cb-4aa2-bbed-c10e4f32b1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists BreadandButter; \n",
    "create database if not exists BreadandButter.Data_Ingestion_DB; \n",
    "create volume if not exists BreadandButter.Data_Ingestion_DB.Data_DE_VL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3d13eef-2b5e-4531-91ef-7587222b7b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "acf0d154-cbfd-441a-9249-7ced7c87ea6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))\n",
    "#inferSchema=True ->Spark scans data and assigns data types automatically\n",
    "#sample(.1) #Randomly selects ~10% of rows -> Data inspection\n",
    "#take(20) -> Displaying first 20 rows (Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93bfeb7-0b1f-40f1-81a2-2881252e18e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "rawdf1.printSchema() #I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "\n",
    "#Column names in exact order\n",
    "print(rawdf1.columns) #I am understanding the column numbers/order and the column names\n",
    "\n",
    "#Column name + datatype mapping\n",
    "print(rawdf1.dtypes) #Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f81f9193-a27b-4484-b9ed-bd9ea10d8397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Identifying all string columns (dynamic logic)\n",
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0])\n",
    "\n",
    "#Full structural metadata\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8efa4159-f6dc-4805-b03b-9b731789ac3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "distinct() and dropDuplicates() both remove duplicate rows across all columns when no subset is provided. \n",
    "- The key difference is that dropDuplicates() allows deduplication based on specific columns, making it more flexible for data engineering use cases.   ex: rawdf1.dropDuplicates(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e42a22a9-f31a-424f-a982-59f6f9c61c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "\n",
    "#Total row count (baseline)\n",
    "print(\"actual count of the data\",rawdf1.count()) \n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(SQL-style operation)\n",
    "print(\"de-duplicated record (all columns) count sqlstyle\",rawdf1.distinct().count())\n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(DataFrame-specific API)\n",
    "print(\"de-duplicated record (all columns) count DF api\",rawdf1.dropDuplicates().count())\n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(remove duplicates based on specific columns)\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())\n",
    "\n",
    "#describe() provides basic statistics like count, mean, min, and max, \n",
    "display(rawdf1.describe())\n",
    "\n",
    "# while summary() extends this by adding percentile-based distribution metrics such as median and quartiles, making it more suitable for deeper data quality analysis.\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7314c5c-36d3-49ac-bfc8-f08658adef5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Active Data Munging** is the continuous process of structuring, validating, cleansing, scrubbing, deduplicating, and standardizing evolving data to make it analytics-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0566832-8227-4d77-899b-713af38b3bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Combining Data + Schema Evolution/Merging (Structuring)\n",
    "- Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "- De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "801cd94c-0d39-473a-afe6-07b74d3b964e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1)**Questions related to multiple files/paths/sub path handling**\n",
    "-->I have data in different filenames in a single/multiple location, i need to read all these data in a df - path=[\"path1/file1\",\"path1/file2\",\"path2/file3\"] I have data in single pattern of file names in a single/multiple locations or subfolders, i need to read all these data in a df - path=[\"path1/\",\"path1/\",\"path2/\"], pathGlobFilter=\"custsm*\", recursiveFileLookup=True\n",
    "\n",
    "2)**Questions related handling evolving data structure with data ingested in different days/periods - Ans. Schema Evolution**\n",
    "Evolution is growth over the time (Filesystem level).. Eg. Source is sending data with additional columns week over week in csv format\n",
    "1. Read and write in Serialized format( ORC,Parquet)\n",
    "2. Read DF with mergeSchema = True\n",
    "\n",
    "3)**Questions related handling data from different sources with different related structure in a same day - Ans. Schema Merging/Melting (Dataframe level)**    Eg. Source1 is sending custsmodified_NY with 5 columns and Source2 is sending custsmodified TX with 4 columns\n",
    "1. Read file1 in DF1, read file2 in DF2\n",
    "2. Create DF3 by merging DF1 and DF2 using df1.unionByName(df2,allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2410378d-1cc7-436b-bbba-347c97802566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified\") \n",
    "#2. Multiple files (with different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified_NY\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/\",\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c1b09f-41f6-45c5-a00a-8f7d933f3a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- When you go for Schema Merging/Melting and Schema Evolution?\n",
    "- Schema Merging/Melting (unionByName,allowMissingColumns)- If we get multiple files\n",
    "- Schema Evolution (orc/parquet with mergeSchema) - If no. of columns are keeps added by the source system\n",
    "- when we know structure of the file already - schema merge/ schema not known earlier  - schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe161708-ca16-4429-a9b4-5cb4f2f3c27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Schema Evolution:\n",
    "Handling schema changes over time in the same dataset.\n",
    "\n",
    "Schema Merging:\n",
    "Combining different schemas from multiple sources into one structure.\n",
    "\n",
    "- If multiple files with different structures arrive together → Schema Merging.\n",
    "- If the same source keeps adding columns over time → Schema Evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ec52906-d80d-4b1f-9c4c-95b1d82e8bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **1. Combining Data + Schema Evolution/Merging (Structuring)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eee3430-c77c-441d-9a82-216ca7638f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema Merging**\n",
    "- Schema Merging is the process of combining data from multiple sources with different schemas at the same point in time into a single unified structure.\n",
    "\n",
    "**Scope**\n",
    "- Same-day / same batch\n",
    "- DataFrame level\n",
    "- _Typical Scenario_\n",
    "- Source A (NY): id, name, email\n",
    "- Source B (TX): id, name, phone\n",
    "\n",
    "**_How it is handled_**\n",
    "- Read separately\n",
    "- Merge using unionByName\n",
    "- df_all = df_ny.unionByName(df_tx, allowMissingColumns=True)\n",
    "\n",
    "\n",
    "What Spark does\n",
    "- Matches columns by name\n",
    "- Adds missing columns as NULL\n",
    "- Produces unified DataFrame\n",
    "\n",
    "Key Point\n",
    "- Schema Merging happens because sources differ, not because time changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0976b393-ec41-428d-a622-62afd9420e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(Important interview question also as like schema evolution...)\n",
    "#4. Multiple files with different structure in multiple paths or sub paths\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf1)\n",
    "display(rawdf2)\n",
    "\n",
    "rawdf_merged=rawdf1.union(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype....Union is position-based ,Same column order, Same number of columns, Same data types.\n",
    "display(rawdf_merged)\n",
    "\n",
    "#Expected right approach to follow #allowMissingColumns=True -> Adds missing columns with NULL\n",
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True) #In unionByName -> Columns matched by name # Missing columns → NULL\n",
    "display(rawdf_merged) #done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2d29844d-1114-4e16-8c88-f2ea144f7480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here, we are merging two files because both are in CSV format. If one file is CSV and the other file is in a different format, what should we do in this scenario? it will be handled automatically\n",
    "#rawdf2.write.json(\"/Volumes/workspace/wd36schema/ingestion_volume/staging/csvjson\")\n",
    "rawdf3=spark.read.json(\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/simple_json.txt\")\n",
    "rawdf_merged=rawdf_merged.unionByName(rawdf3,allowMissingColumns=True)\n",
    "display(rawdf_merged)#Expected dataframe to proceed further munging on a single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3c27fcc-773c-43f1-9e41-b260c9d28947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5be28d74-de56-407d-9ee7-f691402224dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "READ MODE's\n",
    "### 1️⃣ mode='permissive' (DEFAULT)\n",
    "What it does\n",
    "- Reads all records\n",
    "- Corrupt / malformed rows are not dropped\n",
    "- They are placed into a special column called _corrupt_record\n",
    "\n",
    "2️⃣ **mode='dropMalformed'**\n",
    " What it does\n",
    "- Drops malformed rows\n",
    "- No _corrupt_record column\n",
    "\n",
    "3️⃣ **mode='failFast'**\n",
    " What it does\n",
    "- Fails immediately when malformed record is found\n",
    "- Stops job execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4477b77a-2ace-4bb1-9e5e-8ae3b103b1b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "#print(rawdf1.schema)\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\", cleandf1.count())#all rows count\n",
    "display(cleandf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified\",mode='dropMalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\", len(cleandf1.collect()))\n",
    "display(cleandf1)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68a94e14-b7cd-4a71-a200-df677a91457c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2007342-4f59-4521-a335-093c4312451e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method3 best methodology of applying active data munging\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "struttype1 = StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "rawdf1=spark.read.schema(struttype1).csv(path=\"/Volumes/breadandbutter/data_ingestion_db/data_de_vl/custsmodified\",mode='permissive')\n",
    "print(\"allow all data showing the real values\",rawdf1.count())#all rows count\n",
    "display(rawdf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802afb4a-de54-4936-8ff7-a72da5245b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cleansing**\n",
    "It is a process of cleaning/removing/deleting unwanted data\n",
    "### - **na.drop(how=\"any\")** \n",
    "- Drops the entire ROW if ANY column in that row is NULL\n",
    "- Only rows with NO nulls at all survive\n",
    "\n",
    "### - 2️⃣ **na.drop(how=\"any\", subset=[\"id\",\"age\"])**\n",
    "- Drops row ONLY IF id OR age is NULL\n",
    "- Other columns are ignored.\n",
    "\n",
    "- Mode\t              Row dropped when\n",
    "- how=\"any\"\t   ----> drop if At least one column is NULL\n",
    "- how=\"all\"\t   ----> drop if All selected columns are NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a050c9f2-94a8-48db-aae5-5eaa56dc9d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2\n",
    "#Important na functions we can use to do cleansing\n",
    "display(rawdf1.where(\"age is null\")) #raw data before cleansing\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\")#This function will drop any column in a given row with null otherwise this function returns rows with no null columns\n",
    "display(cleanseddf.where(\"age is null\"))#after cleansing no null row data will be seen\n",
    "\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\",subset=[\"id\",\"age\"]) #Drops row ONLY IF id OR age is NULL\n",
    "display(cleanseddf)\n",
    "cleanseddf=rawdf1.na.drop(how=\"all\",subset=[\"lastname\",\"profession\"])#4000004,Gretchen,,66,\n",
    "display(cleanseddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "123dc8f9-89b6-4c59-887b-0d93a2d0b820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Scrubbing**\n",
    " It is a process of polishing/fine tuning/scrubbing/meaningful conversion the data in a usable format\n",
    "### -  **na.fill()** - Replaces NULL values in the specified column(s) with the given value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01f1486-1ac3-4995-a604-1f0ff0684c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf = rawdf1.na.fill('user not provided',subset=[\"lastname\"])#4000004,Gretchen,,66,\n",
    "display(cleanseddf)\n",
    "\n",
    "cleanseddf = rawdf1.na.fill('NA')#replaces all null values with 'NA'\n",
    "display(cleanseddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "965953d0-8c10-4a09-b6cd-90dceb7ec208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1️⃣ Schema Evolution\n",
    "- Schema Evolution is the ability of a system to handle changes in data structure over time as new data arrives with additional or modified columns, without breaking existing pipelines.\n",
    "### Scope\n",
    "- Time-based (day over day / week over week)\n",
    "- Filesystem / table level\n",
    "- _Typical Scenario_\n",
    "- Week 1 file: id, name\n",
    "- Week 2 file: id, name, email\n",
    "- Week 3 file: id, name, email, phone\n",
    "### How it is handled\n",
    "- Use schema-aware formats (Parquet / ORC)\n",
    "- Enable mergeSchema = true at read time\n",
    "- _spark.read.option(\"mergeSchema\", \"true\").parquet(\"/data/customers\")_\n",
    "### What Spark does\n",
    "- Reads schema from all files\n",
    "- Merges them into a superset\n",
    "- Missing columns → NULL\n",
    "- Key Point\n",
    "- Schema Evolution happens because data changes over time."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8580293281368747,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) BreadandButter_program",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
