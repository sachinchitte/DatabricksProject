{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd61e449-57cb-45e0-b1f3-ddde8cf5e3a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Catalog,schema,volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1f6221-89a3-471e-b48a-07b0d263ae9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "create catalog if not exists catalog1_dropme; \n",
    "create database if not exists catalog1_dropme.schema1_dropme; \n",
    "create volume if not exists catalog1_dropme.schema1_dropme.volume1_dropme;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6618938-c301-4c0c-8d65-b6029fb9f931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ## Typical Spark Program\n",
    "### from pyspark.sql.session import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d336e275-0874-42b2-bdf4-2bc74384249e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Component      | Name                | Type       |\n",
    "| -------------- | ------------------- | ---------- |\n",
    "| `pyspark`      | pyspark             | Package    |\n",
    "| `sql`          | pyspark.sql         | Subpackage |\n",
    "| `session`      | pyspark.sql.session | Module     |\n",
    "| `SparkSession` | SparkSession        | Class      |\n",
    "In Python, a package is a directory of modules, a subpackage is a package inside another package, a module is a single .py file, and a class is a blueprint defined inside a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c617f78e-18fb-4729-8efc-29409bc14e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1= SparkSession.builder.getOrCreate()\n",
    "print(spark1)#we instantiated\n",
    "# SparkSession.builder.getOrCreate() \n",
    "#either returns an existing SparkSession or creates one if none exists. In Databricks, it always returns the pre-created session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b8115d0-ea58-407d-8e45-8d13c06860b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Read/Extract the data from the filesytem and load it into the distributed memory for further processing/load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9623b357-1fbb-431e-b951-a4c3e504a704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (default) If we don't use any options in spark.read.csv():\n",
    "1. By default it uses comma ( , ) as delimiter (sep=\",\")\n",
    "2. By default header=False, so first row is treated as data and column names are auto-generated as _c0, _c1, ... _cn\n",
    "3. By default inferSchema=False, so all columns are read as StringType\n",
    "4. Default read mode is PERMISSIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb78407-a2e5-4c81-809a-bd60e7412b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# File path inside the volume\n",
    "file_path = \"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\"\n",
    "\n",
    "# Sample CSV content\n",
    "csv_data = \"\"\"cust_id,cust_name,age,city\n",
    "1,John,30,Hyderabad\n",
    "2,Jane,28,Bangalore\n",
    "3,Robert,35,Pune\n",
    "4,Emily,32,Chennai\n",
    "\"\"\"\n",
    "\n",
    "#Method 1 dbutils.fs.put Part of dbutils.fs utility\n",
    "dbutils.fs.put(\n",
    "    file_path,\n",
    "    csv_data,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Verify file creation\n",
    "dbutils.fs.ls(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa724da-ff1d-40f8-9216-6b1048c1924c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbfs:/// is the URI(Uniform Resource Identifier) prefix for accessing DBFS paths\n",
    "#dbfs:// → scheme(URI scheme (tells Spark/Databricks to use DBFS)) / protocol, like http:// or file:// \n",
    "# / → root directory of the DBFS filesystem\n",
    "\n",
    "csv_df1=spark.read.csv(\"dbfs:////Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "csv_df1.printSchema()\n",
    "display(csv_df1)#display with produce output in a beautified table format, specific to databricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5c67e0-9387-4d57-8866-ca1fd8c6cf56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Header Concepts(2 ways).\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, but we are asking spark to take the first row as header and not as a data?\n",
    "#WAY1\n",
    "csv_df1=spark.read.csv(\"dbfs:////Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\",header=True)\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "#csv_df1.write.csv(\"/Volumes/workspace/wd36schema2/volume1/folder1/outputdata\")\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "#way2\n",
    "csv_df2=spark.read.csv(\"dbfs:////Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\").toDF(\"my_id\",\"name\",\"age\",\"city\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dffaca7-1a9f-4ca3-9c37-71fc1e53e0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Printing Schema (equivalent to describe table)\n",
    "csv_df1.printSchema()\n",
    "csv_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d04eead-8a87-4bc4-bf27-7bc8e4a9dbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Inferring Schema \n",
    "# (Performance Consideration: Use this function causiously because it scans the entire data by immediately evaluating and executing\n",
    "# hence, not good for large data or not good to use on the predefined schema dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f417b5b0-de5c-4677-8661-d3dabbf1d982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading CSV without inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "47c0d096-e7bd-45c3-bc1a-0e4104287e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\n",
    "    \"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\",\n",
    "    header=True        # Use first row as column names\n",
    ")\n",
    "csv_df1.printSchema()\n",
    "csv_df1.show(5)\n",
    "\n",
    "'''\n",
    "Observation without inrferschema\n",
    "1)All columns considers as strings (stringType)\n",
    " |-- cust_id: string (nullable = true)\n",
    " |-- cust_name: string (nullable = true)\n",
    " |-- age: string (nullable = true)\n",
    " |-- city: string (nullable = true)\n",
    "2)Even numeric columns like cust_id and age are strings\n",
    "3)You cannot perform numeric operations (sum, avg, comparison) on age without casting '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69b1f62d-94d6-4f79-8980-c8a511672a48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading CSV with inferSchema=True\n",
    "How inferSchema works internally -->\n",
    "Spark scans a sample of the CSV file (default is first 1000 rows)\n",
    "\n",
    "If type mismatch occurs in the sample, Spark may default to StringType for safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "426c0c84-b9f3-4e9a-80d8-511dfca0e7d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df2 = spark.read.csv(\n",
    "    \"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\",\n",
    "    header=True,       # First row is column names\n",
    "    inferSchema=True   # Spark will detect correct data types\n",
    ")\n",
    "csv_df2.printSchema()\n",
    "csv_df2.show(5)\n",
    "'''\n",
    "Observation\n",
    "1)Spark automatically detected cust_id and age as integers\n",
    "2)cust_name and city remain strings\n",
    "3)Numeric operations are now possible:\n",
    "|-- cust_id: integer (nullable = true)\n",
    " |-- cust_name: string (nullable = true)\n",
    " |-- age: integer (nullable = true)\n",
    " |-- city: string (nullable = true)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebe5671d-ac6b-4482-8b75-8e5e6ea55e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generic way of read and load data into dataframe using fundamental options from built in sources (csv/orc/parquet/xml/json/table) (inferschema, header, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1660d5-bbb5-44a1-b234-1aee0ce32786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\",inferSchema=True,header=True,sep=',')  # 1~john~30~Hyderabad then use sep='~'....default is \" , \n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad21ab5d-fa47-4bed-b794-e5c83b6481a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Provide schema with SQL String or programatically (very very important)\n",
    " Important part - Using structure type to define custom complex schema.\n",
    "import the types library based classes..\n",
    " define_structure=StructType([StructField(\"colname\",DataType(),True),StructField(\"colname\",DataType(),True)...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5380cff-1d30-440a-a48f-b724b82aa1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"id\",IntegerType(),False),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"prof\",StringType())])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1456fee2-e30c-4aaa-b456-f3ab513f1062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ✅ What is Manual Schema?(most used)\n",
    "### \n",
    "### Manual schema means you explicitly define column names, data types, and nullability, instead of letting Spark guess.\n",
    "\n",
    "\n",
    "StructField(\"cust_id\", IntegerType(), **_True_**)\n",
    "Indicates whether this column can contain NULL values\n",
    "**_True_** → NULL allowed\n",
    "**_False_** → NULL NOT allowed (strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ac805b0-5966-44e9-9dc2-80cbd4029085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "# 1️⃣ Define schema explicitly\n",
    "manual_schema = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), True),\n",
    "    StructField(\"cust_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "# 2️⃣ Read file using the schema\n",
    "v_df1 = spark.read.csv(\n",
    "    \"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\",\n",
    "    schema=manual_schema,  # Manual schema applied\n",
    "    header=True,           # First row is header\n",
    "    sep=','                # default , delimiter(if not given no Issue)\n",
    ")\n",
    "# 3️⃣ Validate data\n",
    "v_df1.show(2)\n",
    "v_df1.printSchema()\n",
    "display(v_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca733a29-2a07-4a68-902b-22fa75a9cdab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "getting data from different source systems of different regions (NY, TX, CA) into different landing pad (locations), how to access this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9b977f-408d-4a24-9ff4-8a5d15d269bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_multiple_sources = spark.read.csv(\n",
    "    [\n",
    "        \"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\",\n",
    "        \"/Volumes/workspace/default/usage_metrics/mobile_os_usage.csv\"\n",
    "    ],\n",
    "    inferSchema=True,\n",
    "    header=True,\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "df_multiple_sources.show(4)\n",
    "print(df_multiple_sources.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4459e5c-4493-454b-86ec-aa7d95f20be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_multiple_sources=spark.read.csv(path=[\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/customers.csv\",\"/Volumes/workspace/default/usage_metrics/mobile_os_usage.csv\"],inferSchema=True,header=True,sep=',',pathGlobFilter=\"customers\",recursiveFileLookup=True)\n",
    "#.toDF(\"cid\",\"fn\",\"ln\",\"a\",\"p\")\n",
    "print(df_multiple_sources.count())\n",
    "df_multiple_sources.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "329aac56-8a5f-48eb-89ac-5504d869b67e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataFrame.write (write.csv / write.parquet)\n",
    "Part of Spark DataFrame API\n",
    "Writes structured data from a DataFrame to CSV, Parquet, JSON, Delta\n",
    "Handles large datasets in parallel (distributed)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8157859182348908,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) basic_readops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
