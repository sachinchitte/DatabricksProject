{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e9db29-71e6-4ee6-a139-a2e6adafec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60102b53-da15-4c74-a306-b675d15c78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![logistics](logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c798b032-39ac-4bf6-9992-78159146fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below gdrive and upload into the catalog\n",
    "https://drive.google.com/drive/folders/1J3AVJIPLP7CzT15yJIpSiWXshu1iLXKn?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca9127-636a-4be2-ad66-6af734d83aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73641445-2b65-4138-89f3-44e10449c278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4135931-76a0-4af8-97c6-84bdc67ff57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists EnterpriseFleetAnalytics; \n",
    "create database if not exists EnterpriseFleetAnalytics.Data_Ingestion_DB; \n",
    "create volume if not exists EnterpriseFleetAnalytics.Data_Ingestion_DB.Data_DE_VL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507bfd1d-8d3d-417b-8da8-074150a9eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb7b0d4-889d-4577-938d-a038b40b7654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/enterprisefleetanalytics/data_ingestion_db/data_de_vl/logistics_source1\",header=True,inferSchema=True)\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))\n",
    "#inferSchema=True ->Spark scans data and assigns data types automatically\n",
    "#sample(.1) #Randomly selects ~10% of rows -> Data inspection\n",
    "#take(20) -> Displaying first 20 rows (Action)\n",
    "rawdf2=spark.read.csv(\"/Volumes/enterprisefleetanalytics/data_ingestion_db/data_de_vl/logistics_source2\",header=True,inferSchema=True)\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf2.take(20))\n",
    "display(rawdf2.sample(.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a1337a-54f7-4775-bf06-b7359e9171c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.Analyse the schema, datatypes, columns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3fbcff6-f34e-4ee4-b3cc-ab121555c855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "rawdf1.printSchema() #I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "rawdf2.printSchema() \n",
    "#Column names in exact order\n",
    "print(rawdf1.columns) #I am understanding the column numbers/order and the column names\n",
    "\n",
    "#Column name + datatype mapping\n",
    "print(rawdf1.dtypes) #Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f1c43e-f847-4d97-ae62-6b9ea1e33e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3.Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c16c66-d1cb-4b4e-9293-0af8dde8c983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Total row count (baseline)\n",
    "print(\"actual count of the data\",rawdf1.count()) \n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(SQL-style operation)\n",
    "print(\"de-duplicated record (all columns) count sqlstyle\",rawdf1.distinct().count())\n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(DataFrame-specific API)\n",
    "print(\"de-duplicated record (all columns) count DF api\",rawdf1.dropDuplicates().count())\n",
    "\n",
    "#de duplicate the entire columns of the given  dataframe(remove duplicates based on specific columns)\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['shipment_id']).count())\n",
    "\n",
    "#describe() provides basic statistics like count, mean, min, and max, \n",
    "display(rawdf1.describe())\n",
    "\n",
    "# while summary() extends this by adding percentile-based distribution metrics such as median and quartiles, making it more suitable for deeper data quality analysis.\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de27b31f-de9a-457b-be06-00ada4960419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:\n",
    "3. fewer columns than expected\n",
    "4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f97700-cae1-4ab6-97e8-502ffbd1a6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session Object\n",
    "#Create a Spark Session Object\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e0d76f-c52b-4ed3-9cd6-9b20e0634000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Without modifying the data, identify:Shipment IDs that appear in both master_v1 and master_v2\n",
    "#Set operations (INTERSECT, UNION) require same datatype\n",
    "\n",
    "common_shipmentsdf = (rawdf1.select(col(\"shipment_id\").cast(\"string\"))\n",
    "          .intersect(rawdf2.select(col(\"shipment_id\").cast(\"string\")))\n",
    "           )\n",
    "\n",
    "display(common_shipmentsdf)\n",
    "common_shipmentsdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e28503-aecf-463a-a762-4abe51baa94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-**Records where shipment_id is non-numeric**\n",
    "- rlike(\"^[0-9]+$\") is a regular-expression (regex) check used in Spark to test whether a value is purely numeric\n",
    "- Keeps rows like: 123, 4567\n",
    "- ‚ùå Drops rows like: ten, 12A, 12.5, NULL\n",
    "- ~col(\"shipment_id\").rlike(\"^[0-9]+$\") --shows 'ten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c68ed17a-bde4-43b5-8f13-9fee80edc2fd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768483872546}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Records where shipment_id is non-numeric #both datasets\n",
    "non_numeric_shipment_Bothdf = (\n",
    "    rawdf1.select((\"shipment_id\"))\n",
    "    .unionByName(rawdf2.select(col(\"shipment_id\").cast(\"string\")))) #combined DataFrame for shipment\n",
    "\n",
    "non_numeric_df=(non_numeric_shipment_Bothdf\n",
    "    .filter(~col(\"shipment_id\").rlike(\"^[0-9]+$\"))) #find non-numeric shipment IDs\n",
    "display(non_numeric_df)\n",
    "\n",
    "#age is not an integer\n",
    "invalid_age_df = (\n",
    "    rawdf1.select(\"age\")\n",
    "    .unionByName(rawdf2.select(\"age\"))) #combined DataFrame for age\n",
    "invalid_age_df=invalid_age_df.filter(~col(\"age\").rlike(\"^[0-9]+$\")) #age is not an integer\n",
    "display(invalid_age_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79effd23-ab37-4ea2-a2b0-eb9b38804c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Count rows having:\n",
    "\n",
    "- 1.fewer columns than expected\n",
    "- 2.more columns than expected\n",
    "- split(col(\"value\"), \",\")\n",
    "- Splits the raw CSV line using comma ,\n",
    "- Converts the string into an array -Size() - Counts how many elements are in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7804c6af-a70b-4110-bd3e-4471c668a282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, size, col\n",
    "#Split and count columns per row\n",
    "rawdf1_with_count = rawdf1.withColumn(\n",
    "    \"col_count\", size(split(col(\"value\"), \",\"))\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6095b785-27a9-4cfb-bdd4-8d6bb406a49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Once data is loaded into a DataFrame:\n",
    "- Spark enforces schema\n",
    "- Every row has the same number of columns\n",
    "- üëâ So you CANNOT detect ‚Äúfewer or more columns per row‚Äù from rawdf1 directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd36d4f4-b10c-4d15-a804-ef4addc68ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected_col_countdf = len(rawdf1.columns)\n",
    "print(\"Expected column count:\", expected_col_countdf)\n",
    "\n",
    "raw_text_df = spark.read.text(\n",
    "    \"/Volumes/enterprisefleetanalytics/data_ingestion_db/data_de_vl/logistics_source1\"\n",
    ")\n",
    "rawdf1_with_count = raw_text_df.withColumn(\n",
    "    \"col_count\", size(split(col(\"value\"), \",\"))\n",
    ")\n",
    "from pyspark.sql.functions import *\n",
    "#3Ô∏è‚É£ Fewer columns than expected\n",
    "fewer_columns_count = (\n",
    "    rawdf1_with_count.where(col(\"col_count\") < expected_col_countdf)\n",
    "        .count()\n",
    ")\n",
    "print(\"Rows with fewer columns than expected:\", fewer_columns_count)\n",
    "#4Ô∏è‚É£ More columns than expected\n",
    "more_columns_count = (\n",
    "rawdf1_with_count\n",
    "        .where(col(\"col_count\") > expected_col_countdf)\n",
    "        .count()\n",
    ")\n",
    "print(\"Rows with fewer columns than expected:\", more_columns_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81fc736-6e9f-4093-9f40-8e047989b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b773a5-b1db-4b1f-bf3e-93067d0483ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebf5d21-3c1d-49c5-9ed3-de21aa0e9bad",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768486946519}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Combining Data + Schema Merging (Structuring)\n",
    "#Read both files without enforcing schema\n",
    "rawdf1 = spark.read.csv(\n",
    "    \"/Volumes/enterprisefleetanalytics/data_ingestion_db/data_de_vl/logistics_source1\",\n",
    "    header=True,\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "rawdf2 = spark.read.csv(\n",
    "    \"/Volumes/enterprisefleetanalytics/data_ingestion_db/data_de_vl/logistics_source2\",\n",
    "    header=True,\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "#Add data_source column with values as: system1, system2 in the respective dataframes\n",
    "df_system1 = rawdf1.withColumn(\"data_source\", lit(\"system1\"))\n",
    "df_system2 = rawdf2.withColumn(\"data_source\", lit(\"system2\"))\n",
    "df_system1.printSchema()\n",
    "df_system2.printSchema()\n",
    "#merged df\n",
    "rawdf_merged = df_system1.unionByName(\n",
    "    df_system2,\n",
    "    allowMissingColumns=True\n",
    ") #Missing columns auto-filled with NULL\n",
    "\n",
    "#Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source\n",
    "canonical_cols = [\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\",\"hub_location\",\"vehicle_type\",\"data_source\"]\n",
    "final_df = rawdf_merged.select(canonical_cols)\n",
    "final_df.printSchema()\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628e4769-0e24-481b-8b5c-33204a91ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68447690-729c-4347-8513-d07f91577642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "print(\"RawDF1 count (before cleaning):\", final_df.count())\n",
    "cleanseddf1 = final_df.na.drop(how=\"any\",subset=[\"shipment_id\", \"role\"])\n",
    "print(\"RawDF1 count (after cleaning):\", cleanseddf1.count())\n",
    "display(cleanseddf1)\n",
    "#Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "print(\"RawDF1 count (before cleaning):\", cleanseddf1.count())\n",
    "cleanseddf2 = cleanseddf1.na.drop(how=\"any\",subset=[\"first_name\", \"last_name\"])\n",
    "print(\"RawDF1 count (after cleaning):\", cleanseddf2.count())\n",
    "display(cleanseddf2)\n",
    "#Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "print(\"RawDF1 count (before cleaning):\", cleanseddf2.count())\n",
    "cleanseddf3 = cleanseddf2.na.drop(how=\"any\",subset=[\"shipment_id\"])\n",
    "print(\"RawDF1 count (after cleaning):\", cleanseddf3.count())\n",
    "display(cleanseddf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f557ac71-1cf5-4641-8b63-4fe0e2f29721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-->na.fill is fast and simple for replacing true NULLs, but it cannot handle malformed or empty values, so it should be used only after data normalization.\n",
    "\n",
    "- NULL ‚Üí replaced\n",
    "- \"\" ‚Üí NOT replaced\n",
    "- \"null\" ‚Üí NOT replaced\n",
    "- \"NA\" ‚Üí NOT replaced\n",
    "- \"ten\" ‚Üí NOT replaced\n",
    "\n",
    "üîπ na.fill() ‚Äî fill NULL values only\n",
    "- What it does\n",
    "- Replaces NULL values\n",
    "- Does not touch empty strings \"\" or other values\n",
    "### üîπ na.replace() ‚Äî replace specific values\n",
    "- df.na.replace({old_value: new_value}, subset=[\"col\"])\n",
    "- What it does\n",
    "- Replaces exact values you specify\n",
    "- Works on non-NULL values\n",
    "- Does not replace NULLs\n",
    "| Scenario             | Use              |\n",
    "| -------------------- | ---------------- |\n",
    "| NULL values          | `na.fill()`      |\n",
    "| Known invalid values | `na.replace()`   |\n",
    "| Both                 | `fill ‚Üí replace` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ddc88b-e667-4a07-83d1-e8382906f5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "#cleanseddf4=cleanseddf3.na.fill(-1,subset=[\"age\"])\n",
    "#---display(cleanseddf4) # only works for for true NULL. dierct check carefully does not work for empty string,null,NA,ten\n",
    "#Since age is a string column, fill it with \"-1\" (string)\n",
    "cleanseddf5 = cleanseddf3.na.fill( {\"age\": \"-1\"} )\n",
    "display(cleanseddf5)\n",
    "#Because the dataset was ingested without schema enforcement, numeric fields like age are strings. I defaulted nulls using string-safe values to avoid implicit casting.\n",
    "\n",
    "#Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "cleanseddf6 = cleanseddf5.na.fill({\"vehicle_type\": \"UNKNOWN\"}\n",
    ")\n",
    "display(cleanseddf6)\n",
    "#Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "cleanseddf7 = cleanseddf6.na.replace({\"ten\": \"-1\", \"\": \"-1\"},subset=[\"age\"])\n",
    "display(cleanseddf7)\n",
    "#Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "cleanseddf8 = cleanseddf7.na.replace({\"truck\": \"LMV\", \"bike\": \"TwoWheeler\"},subset=[\"vehicle_type\"])\n",
    "display(cleanseddf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b089e58-4b74-41e5-b050-bbfa8d249467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc60af7-9398-4de0-93ce-1f5bf9dd5c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation <br>\n",
    "1. Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7d6d5b-a7dd-4c41-a858-d41e73e87a33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Use multiline = true for .json() to read the json file because the json file is multiline\n",
    "shipment_df = (\n",
    "    spark.read\n",
    "        .option(\"multiline\", \"true\")   # ‚≠ê critical\n",
    "        .json(\"/Volumes/enterprisefleetanalytics/data_ingestion_db/data_de_vl/logistics_shipment_detail_3000.json\")\n",
    ")\n",
    "shipment_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd9b438-99be-431d-a81a-493c23b2b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02c8ce4b-25a0-4eb4-8b09-ea05938822c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add a columnSource File: logistics_shipment_detail_3000.json : domain as 'Logistics' Add a column\n",
    "#Source File: DF of logistics_shipment_detail_3000.json\n",
    "#: domain as 'Logistics', current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "shipment_df1 = (shipment_df\n",
    "                .withColumn(\"domain\", lit(\"Logistics\"))\n",
    "                .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "                .withColumn(\"is_expedited\", lit(False))\n",
    "            )\n",
    "shipment_df1.printSchema()\n",
    "#Column Uniformity: role - Convert to lowercase\n",
    "#Source File: logistics_source1 & logistics_source2\n",
    "#vehicle_type - Convert values to UPPERCASE\n",
    "cleanseddf9= (\n",
    "        cleanseddf8.withColumn(\"role\", lower(col(\"role\")))\n",
    "        .withColumn(\"vehicle_type\", upper(col(\"vehicle_type\")))\n",
    ") # or latest cleansed df for source1\n",
    "display(cleanseddf9)\n",
    "#Source Files: logistics_shipment_detail_3000.json (and the merged master files) hub_location - Convert values to initcap case\n",
    "cleanseddf10 = cleanseddf9.withColumn(\"hub_location\", initcap(col(\"hub_location\"))) #new york ‚Üí New York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbaa2061-ae4d-4091-b353-8ea630affd77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Format Standardization:\n",
    "- Source Files: DF of logistics_shipment_detail_3000.json\n",
    "- Convert shipment_date to yyyy-MM-dd\n",
    "- Ensure shipment_cost has 2 decimal precision\n",
    "#to_date(col(\"shipment_date\"), \"dd-MM-yy\")\n",
    "> What it does -Parses the STRING into a Spark DATE -->Uses the explicit format: dd-MM-yy\n",
    "# date_format(‚Ä¶, \"yyyy-MM-dd\")\n",
    "> What it does --Takes a DATE and formats it as a STRING--Format specified: yyyy-MM-dd\n",
    "- STRING (shipment_date)\n",
    "-    ‚Üì to_date(\"dd-MM-yy\")\n",
    "- DATE\n",
    "-    ‚Üì date_format(\"yyyy-MM-dd\")\n",
    "- STRING (formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79c73a9-6d85-48e5-8686-fafeb380faf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Step   | Function        | Purpose       |\n",
    "| ------ | --------------- | ------------- |\n",
    "| Parse  | `to_date()`     | STRING ‚Üí DATE |\n",
    "| Format | `date_format()` | DATE ‚Üí STRING |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf92a31e-86db-4e1d-8a86-577b11c3f624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Convert to STRING formatted as yyyy-MM-dd\n",
    "\n",
    "shipment_df2 = shipment_df1.withColumn(\"shipment_date\",date_format(to_date(col(\"shipment_date\"), \"dd-MM-yy\"),\"yyyy-MM-dd\"))\n",
    "display(shipment_df2)\n",
    "#Ensure shipment_cost has 2 decimal precision\n",
    "shipment_df3 = shipment_df2.withColumn(\n",
    "    \"shipment_cost\",\n",
    "    round(col(\"shipment_cost\"), 2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a4dbd8-b561-4bd6-857a-b6f5d3f1959f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Type Standardization\n",
    "- Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "- Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "- age: Cast String to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10480154-39a3-440e-808f-dace0aa6831e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768505451899}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#age: Cast String to Integer\n",
    "cleanseddf11 = cleanseddf10.withColumn(\"age\",col(\"age\").cast(\"int\"))\n",
    "cleanseddf11.printSchema()\n",
    "\n",
    "#Source File: DF of logistics_shipment_detail_3000.json\n",
    "#shipment_weight_kg: Cast to Double\n",
    "shipment_df4 = shipment_df3.withColumn(\n",
    "    \"shipment_weight_kg\",\n",
    "    col(\"shipment_weight_kg\").cast(\"double\")\n",
    ")\n",
    "#is_expedited: Cast to Boolean\n",
    "shipment_df5 = shipment_df4.withColumn(\n",
    "    \"is_expedited\",\n",
    "    col(\"is_expedited\").cast(\"boolean\")\n",
    ")\n",
    "shipment_df5.printSchema()\n",
    "display(shipment_df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bba9e9c-340e-4bc2-bbc9-c8ba04da7d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#withColumnRenamed #Rename: first_name to staff_first_name #Rename: last_name to staff_last_name\n",
    "#Rename: hub_location to origin_hub_city\n",
    "cleanseddf12 = (\n",
    "    cleanseddf11\n",
    "        .withColumnRenamed(\"first_name\", \"staff_first_name\")\n",
    "        .withColumnRenamed(\"last_name\", \"staff_last_name\")\n",
    "        .withColumnRenamed(\"hub_location\", \"origin_hub_city\")\n",
    ")\n",
    "\n",
    "cleanseddf12.printSchema()\n",
    "display(cleanseddf12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e05d6e2-a188-406b-a7fc-b1a95478d141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Reordering columns logically in a better standard format:\n",
    "- Source File: DF of Data from all 3 files\n",
    "- shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e894405-1a23-4ec8-a7be-2965c4142932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "cleanseddf12.printSchema()\n",
    "shipment_df5.printSchema()\n",
    "cleanseddf12_str = cleanseddf12.withColumn(\n",
    "    \"shipment_id\",\n",
    "    col(\"shipment_id\").cast(\"string\")\n",
    ")\n",
    "\n",
    "shipment_df5_str = shipment_df5.withColumn(\n",
    "    \"shipment_id\",\n",
    "    col(\"shipment_id\").cast(\"string\")\n",
    ")\n",
    "\n",
    "union_df = cleanseddf12_str.unionByName(shipment_df5_str, allowMissingColumns=True)\n",
    "final_ordered_df = union_df.select(\n",
    "    \"shipment_id\",          # Identifier\n",
    "    \"staff_first_name\",     # Dimension\n",
    "    \"staff_last_name\",      # Dimension\n",
    "    \"role\",                 # Dimension\n",
    "    \"origin_hub_city\",      # Location\n",
    "    \"shipment_cost\",        # Metric\n",
    "    \"ingestion_timestamp\"   # Audit\n",
    ")\n",
    "\n",
    "final_ordered_df.printSchema()\n",
    "display(final_ordered_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bafa5c8-7355-4d4f-9caf-40e0e0d303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6812baa-d79c-4b7d-bd7a-6d29d3243e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply Record Level De-Duplication\n",
    "recordleveldup_df = union_df.dropDuplicates()\n",
    "#Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "duplicate_removed= recordleveldup_df.dropDuplicates([\"shipment_id\"])\n",
    "#using window function for deduplication removal\n",
    "from pyspark.sql.window import Window\n",
    "pk_window = (\n",
    "    Window\n",
    "    .partitionBy(\"shipment_id\")\n",
    "    .orderBy(col(\"ingestion_timestamp\").desc())\n",
    ")\n",
    "# 3. Keep latest record per shipment_id\n",
    "final_df = (\n",
    "    duplicate_removed\n",
    "    .withColumn(\"rn\", row_number().over(pk_window))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184ecd12-8081-4c8d-925e-d47c84e520d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d404f-b7d4-4e56-9f09-9367ec05e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be734f2-bb7c-45ff-85ff-c7f270539afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a column load_dt using the function current_timestamp()\n",
    "cleanseddf13 = cleanseddf12_str.withColumn(\n",
    "    \"load_dt\",\n",
    "    current_timestamp()\n",
    ")\n",
    "#Create full_name by concatenating first_name and last_name with a space separator.\n",
    "#concat_ws() automatically handles NULLs\n",
    "#Prevents results like \"Rajesh null\"\n",
    "#Cleaner than concat() #concat_ws = best way to concatenate columns\n",
    "cleanseddf14 = cleanseddf13.withColumn(\n",
    "    \"full_name\",\n",
    "    concat_ws(\" \", col(\"staff_first_name\"), col(\"staff_last_name\"))\n",
    ")\n",
    "#Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "display(cleanseddf14)\n",
    "shipment_df6 = shipment_df5_str.withColumn(\n",
    "    \"route_segment\",\n",
    "    concat_ws(\"-\", col(\"source_city\"), col(\"destination_city\"))\n",
    ")\n",
    "#Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\"\n",
    "shipment_df7 = shipment_df6.withColumn(\n",
    "    \"vehicle_identifier\",\n",
    "    concat_ws(\"_\", col(\"vehicle_type\"), col(\"shipment_id\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e525645-18d1-4a16-9909-afc89a2ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**\n",
    "\n",
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)\n",
    "\n",
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday.\n",
    "\n",
    "**4. Flag shipment status (`is_expedited`)**\n",
    "* **Scenario:** The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "* **Action:** Flag as **'True'** if the `shipment_status` IN_TRANSIT or DELIVERED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "230d3b6a-1738-4a2c-b594-0ea6e88fcb33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Result: \"2024-04-23\" -> 2024\n",
    "shipment_df6 = shipment_df5.withColumn(\n",
    "    \"shipment_year\",\n",
    "    year(to_date(col(\"shipment_date\"), \"yyyy-MM-dd\"))\n",
    ")\n",
    "shipment_df6.printSchema()\n",
    "\n",
    "shipment_df7 = shipment_df6.withColumn(\n",
    "    \"shipment_month\",\n",
    "    date_format(to_date(col(\"shipment_date\"), \"yyyy-MM-dd\"), \"MMM\") #month in Jan,Feb format\n",
    ")\n",
    "display(shipment_df7)\n",
    "shipment_df8 = shipment_df7.withColumn(\"is_weekend\",\n",
    "    when(dayofweek(to_date(col(\"shipment_date\"), \"yyyy-MM-dd\")).isin(1, 7),True).otherwise(False))\n",
    "display(shipment_df8)\n",
    "shipment_df9 = shipment_df8.withColumn(\"is_expedited\",\n",
    "    col(\"shipment_status\").isin(\"IN_TRANSIT\", \"DELIVERED\")\n",
    ")\n",
    "display(shipment_df9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f178441f-3675-448e-b8f1-f45336851f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "\n",
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`\n",
    "\n",
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`\n",
    "\n",
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9bd6d8e-20d8-441c-b836-83c46cbc8ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Unit Cost (cost_per_kg)\n",
    "shipment_df10 = shipment_df9.withColumn(\n",
    "    \"cost_per_kg\",\n",
    "    when(col(\"shipment_weight_kg\") > 0,\n",
    "         col(\"shipment_cost\") / col(\"shipment_weight_kg\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "#Track Shipment Age (days_since_shipment)\n",
    "shipment_df11 = shipment_df10.withColumn(\n",
    "    \"days_since_shipment\",\n",
    "    datediff(current_date(), col(\"shipment_date\"))\n",
    ")\n",
    "shipment_df12 = shipment_df11.withColumn(\n",
    "    \"tax_amount\",\n",
    "    round(col(\"shipment_cost\") * 0.18, 2)\n",
    ")\n",
    "display(shipment_df12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de85d4f-d903-46fd-b110-1476a2383d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6686cf0-b06e-40f3-a0f2-9cfee9880c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf15 = cleanseddf14.drop(\n",
    "    \"staff_first_name\",\n",
    "    \"staff_last_name\"\n",
    ")\n",
    "cleanseddf15.printSchema()\n",
    "display(cleanseddf15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7682d4f7-a188-4f86-b60f-c1afa5db220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Splitting & Merging/Melting of Columns\n",
    "*Reshaping columns to extract hidden values or combine fields for better analysis.*<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)\n",
    "\n",
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709798a4-ede3-43b2-927b-36a58edb6ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- How **translate()** works\n",
    "- **translate(column, chars_to_remove, \"\")**\n",
    "- Removes all matching characters from the string\n",
    "- Works character-by-character, not pattern-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3874c37d-d185-4ff4-9dca-808a913dabfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#translate(column, chars_to_remove, \"\")\n",
    "#Action: Split order_id (\"ORD100000\") into two new columns\n",
    "shipment_df13 = (\n",
    "    shipment_df12.withColumn(\"order_prefix\",translate(col(\"order_id\"), \"0123456789\", \"\"))\n",
    "        .withColumn(\"order_sequence\",translate(col(\"order_id\"), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"\"))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21fc4e7b-d62f-41a7-9440-05e5873eb924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- üß† How regexp_extract() works\n",
    "- Uses regular expressions\n",
    "- Extracts substrings matching a pattern\n",
    "- Index 0 = full regex match\n",
    "- Regex explained\n",
    "- ^[A-Za-z]+ ‚Üí letters at the start\n",
    "- [0-9]+$ ‚Üí digits at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfcbd454-0726-476d-9767-0eb269f56c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shipment_df14 = (\n",
    "    shipment_df13.withColumn(\n",
    "            \"order_prefix\",\n",
    "            regexp_extract(col(\"order_id\"), \"^[A-Za-z]+\", 0)\n",
    "        )\n",
    "        .withColumn(\"order_sequence\",\n",
    "            regexp_extract(col(\"order_id\"), \"[0-9]+$\", 0)\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06108658-4af0-4c0d-b027-fba0dc6b745d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ingestion_timestamp\":271},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768662047520}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Action: Split shipment_date into three separate columns for partitioning:\n",
    "shipment_df15 = shipment_df14.withColumn(\n",
    "    \"shipment_date\",\n",
    "    to_date(col(\"shipment_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "#Action: Split shipment_date into three separate columns for partitioning\n",
    "shipment_df16 = (\n",
    "    shipment_df15\n",
    "        .withColumn(\"ship_year\", year(col(\"shipment_date\")))\n",
    "        .withColumn(\"ship_month\", month(col(\"shipment_date\")))\n",
    "        .withColumn(\"ship_day\", dayofmonth(col(\"shipment_date\")))\n",
    ")\n",
    "#Action: Merge source_city (\"Chennai\") and destination_city (\"Pune\") to create a descriptive route key\n",
    "shipment_df17 = shipment_df16.withColumn(\n",
    "    \"route_lane\",\n",
    "    concat_ws(\"->\", col(\"source_city\"), col(\"destination_city\"))\n",
    ")\n",
    "display(shipment_df16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a4891e-0c24-40b4-8ed8-b124efed02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754ff228-9421-49f0-a4c4-df0bb884f4f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-Why Python UDFs are slow \n",
    "> 1.JVM ‚Üî Python serialization\n",
    "- Spark runs in JVM.\n",
    "- Python UDFs run in Python workers.\n",
    "> 2.Spark cannot optimize UDF logic\n",
    "- Spark SQL functions:\n",
    "- Are Catalyst-optimized\n",
    "- Use code generation\n",
    "- Can be reordered, pushed down, pruned\n",
    "> 3.Python UDFs:\n",
    "- Treated as black boxes\n",
    "- No predicate pushdown\n",
    "- No column pruning\n",
    "- No whole-stage codegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5cf85b-ec01-4ad2-b647-6fc4a9792de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#UDF1 Action: Create a Python function calculate_bonus(role, age) and register it as a Spark UDF.\n",
    "def calculate_bonus(role, age):\n",
    "    if role == \"driver\" and age > 50:\n",
    "        return 0.15\n",
    "    elif role == \"driver\" and age < 30:\n",
    "        return 0.05\n",
    "    else:\n",
    "        return 0.0\n",
    "#from pyspark.sql.functions import udf\n",
    "calculate_bonus_udf = udf(calculate_bonus)\n",
    "cleanseddf16 = cleanseddf15.withColumn(\n",
    "    \"projected_bonus\",\n",
    "    calculate_bonus_udf(col(\"role\"), col(\"age\"))\n",
    ")\n",
    "display(cleanseddf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dedd75ac-f875-4549-becc-d74cbd4d46b0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768664771746}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Input  ‚Üí \"Rajesh\"\n",
    "#Output ‚Üí \"Ra****h\"\n",
    "def mask_identity(name):\n",
    "    if name is None:\n",
    "        return None\n",
    "    if len(name) <= 3:\n",
    "        return name  # too short to mask meaningfully\n",
    "    return name[:2] + \"****\" + name[-1]\n",
    "mask_identity_udf = udf(mask_identity, StringType())\n",
    "\n",
    "cleanseddf17 = cleanseddf16.withColumn(\n",
    "    \"masked_name\",\n",
    "    mask_identity_udf(col(\"full_name\"))\n",
    ")\n",
    "display(cleanseddf17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed3d160-f54e-4bf3-bb2c-40ae7ff1b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n",
    "\n",
    "**1. Select (Projection)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n",
    "**2. Filter (Selection)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`.\n",
    "\n",
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday.\n",
    "\n",
    "**4. Format (Standardization)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"‚Çπ30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\" ‚Üí **\"CHENNAI\"**).\n",
    "\n",
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`.\n",
    "\n",
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending).\n",
    "\n",
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72036083-aeaf-4ddd-bf4a-78e77869771c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf17_driver_view = cleanseddf11.select(\n",
    "    \"first_name\",\n",
    "    \"role\",\n",
    "    \"hub_location\"\n",
    ")\n",
    "display(cleanseddf17_driver_view)\n",
    "#shipment_df17\n",
    "#Filter rows where shipment_status is 'DELAYED' or 'RETURNED'.\n",
    "operational_issues_df = shipment_df17.filter(\n",
    "    col(\"shipment_status\").isin(\"DELAYED\", \"RETURNED\")\n",
    ")\n",
    "#Filter rows where age > 50\n",
    "senior_staff_df = cleanseddf17.filter(\n",
    "    col(\"age\") > 50\n",
    ")\n",
    "display(senior_staff_df)\n",
    "#is_high_value = True if shipment_cost > 50,000, else False\n",
    "shipment_df17 = shipment_df17.withColumn(\n",
    "    \"is_high_value\",\n",
    "    when(col(\"shipment_cost\") > 50000, True).otherwise(False)\n",
    ")\n",
    "\n",
    "#is_weekend = True if shipment day is Saturday or Sunday\n",
    "shipment_df17 = shipment_df17.withColumn(\n",
    "    \"is_weekend\",\n",
    "    when(dayofweek(col(\"shipment_date\")).isin(1, 7), True).otherwise(False)\n",
    ")\n",
    "display(shipment_df17)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ae36fb-810c-4d43-b673-76a9c3988a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- format_number(col, 2) ‚Üí adds commas + 2 decimals\n",
    "- concat() ‚Üí safely prepends currency symbol\n",
    "- Returns STRING (ideal for reports, dashboards, exports)\n",
    "- **- groupBy().agg(count())**\n",
    "- **groupBy().agg(sum())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e261fa-d79d-4c9a-9ed8-6688501895bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#Format shipment_cost to a string like: ‚Çπ30,695.80\n",
    "#chennai ‚Üí CHENNAI\n",
    "df = (shipment_df17.withColumn(\n",
    "            \"shipment_cost_fmt\",\n",
    "            concat(lit(\"‚Çπ\"), format_number(col(\"shipment_cost\"), 2))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"source_city_std\",\n",
    "            upper(col(\"source_city\"))\n",
    "        )\n",
    ")\n",
    "display(df)\n",
    "#Group by hub_location and count number of staff (bestway)\n",
    "staff_by_hub = cleanseddf11.groupBy(\"hub_location\") \\\n",
    "    .agg(\n",
    "        count(col(\"staff_first_name\")).alias(\"staff_count\")\n",
    "    )\n",
    "#secondway\n",
    "staff_by_hub = cleanseddf11.groupBy(\"hub_location\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"staff_count\")\n",
    "display(staff_by_hub)\n",
    "#Group by vehicle_type and sum shipment_weight_kg\n",
    "fleet_capacity = shipment_df17.groupBy(\"vehicle_type\") \\\n",
    "    .agg(sum(\"shipment_weight_kg\").alias(\"total_weight_kg\"))\n",
    "\n",
    "expensive_shipments_df = shipment_df17.orderBy(col(\"shipment_cost\").desc())\n",
    "dispatch_schedule_df = shipment_df17.orderBy(\"shipment_date\")\n",
    "#Action: Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows.\n",
    "critical_delays_df = (\n",
    "    shipment_df17\n",
    "        .filter(col(\"shipment_status\") == \"DELAYED\")\n",
    "        .orderBy(col(\"shipment_cost\").desc())\n",
    "        .limit(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992924-2d11-4cfa-b8fa-5c96bf6d0475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*\n",
    "\n",
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> DF of logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> DF of logistics_shipment_detail_3000.json<br>\n",
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n",
    "\n",
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n",
    "\n",
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`.\n",
    "\n",
    "### **2. Lookup**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the corporate `Master_City_List`.\n",
    "* **Action:** Compare values against a reference list.\n",
    "\n",
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding `lat` and `long` columns for map plotting.\n",
    "\n",
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: DF of All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`.\n",
    "\n",
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment.\n",
    "\n",
    "### **7. Set Operations**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires).\n",
    "\n",
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "DF of logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a7f4f5-752c-4fbd-a546-cacf44e3e39a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.1 Frequently Used Simple Joins (Inner, Left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375acb5d-d3a0-4226-856d-21c63893d73a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768723192917}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768723192943}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scenario: We only want to analyze completed work. Connect Staff to the Shipments they handled.\n",
    "#Result: Returns only rows where a staff member is assigned to a valid shipment.\n",
    "staff_df = cleanseddf12_str.alias(\"staff\")\n",
    "shipments_df = shipment_df5_str.alias(\"ship\")\n",
    "display(staff_df)\n",
    "display(shipments_df)\n",
    "performance_df = staff_df.join(\n",
    "    shipments_df,\n",
    "    staff_df.shipment_id == shipments_df.shipment_id,\n",
    "    \"inner\"\n",
    ")\n",
    "display(performance_df)\n",
    "#Scenario: Find out which staff members are currently idle (not assigned to any shipment).\n",
    "#Action: Join staff_df (Left) with shipments_df (Right) on shipment_id. Filter where shipments_df.shipment_id is NULL.\n",
    "idle_staff_df = (staff_df.join(shipments_df,staff_df.shipment_id == shipments_df.shipment_id,\"left\")\n",
    "                .filter(col(\"ship.shipment_id\").isNull()))\n",
    "display(idle_staff_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048bfff2-ec07-48e9-87e1-91a20ad4e567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)\n",
    "- Goal: Find pairs of employees working in the same hub_location\n",
    "- Join Type: SELF JOIN\n",
    "- Condition:\n",
    "- Same hub_location\n",
    "- Different employees ‚Üí staff_id_A != staff_id_B\n",
    "\n",
    "/*Self Join (Peer Finding):\n",
    "Scenario: Find all pairs of employees working in the same hub_location.*/\n",
    "Normal code flow if we have correct data\n",
    "- a = staff_df.alias(\"a\")\n",
    "- b = staff_df.alias(\"b\")\n",
    "- peer_df = (a.join (b, on=col(\"a.hub_location\") == col(\"b.hub_location\"), how=\"inner\"). filter(col(\"a.staff_id\") != col(\"b.staff_id\")))\n",
    "> If you only want unique pairs (Alice‚ÄìBob but not Bob‚ÄìAlice):\n",
    ">  .filter(col(\"a.staff_id\") < col(\"b.staff_id\"))\n",
    "| Method     | Output         |\n",
    "| ---------- | -------------- |\n",
    "| row_number | 1,2,3,4        |\n",
    "| mono ID    | 0,1,8589934592 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6e532f-e1f4-469e-bbe6-b11cce9de249",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768724397717}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf11_str = cleanseddf11.withColumn(\n",
    "    \"shipment_id\",\n",
    "    col(\"shipment_id\").cast(\"string\")\n",
    ")\n",
    "#cleanseddf99_str = cleanseddf11_str.withColumn(\"staff_id\", monotonically_increasing_id()) #IDs are unique but not continuous\n",
    "#display(cleanseddf99_str)\n",
    "window_spec = Window.orderBy(\"shipment_id\")\n",
    "cleanseddf12_str = cleanseddf11_str.withColumn(\"staff_id\", row_number().over(window_spec))\n",
    "display(cleanseddf12_str)\n",
    "staff_A = cleanseddf12_str.alias(\"A\") #self join\n",
    "staff_B = cleanseddf12_str.alias(\"B\")\n",
    "peer_df = (staff_A.join(staff_B,col(\"A.hub_location\") == col(\"B.hub_location\"),\"inner\")\n",
    "         .filter(col(\"A.staff_id\") != col(\"B.staff_id\"))) #staff_id_A not found in dataset so used shipment_id\n",
    "display(peer_df)\n",
    "#Avoid mirrored duplicate pairs (A‚ÄìB and B‚ÄìA)\n",
    "#.filter(col(\"A.staff_id\") < col(\"B.staff_id\"))\n",
    "peer_df1 = (staff_A.join(staff_B,col(\"A.hub_location\") == col(\"B.hub_location\"),\"inner\")\n",
    "         .filter(col(\"A.staff_id\") < col(\"B.staff_id\"))) #staff_id_A not found in dataset so used shipment_id\n",
    "display(peer_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ac2dc0-94d2-4ad7-93da-1ee3c84f4259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Right Join (Orphan Data Check):\n",
    "- Scenario: Identify shipments in the system that have no valid driver assigned (Data Integrity Issue).\n",
    "- Action: Join staff_df (Left) with shipments_df (Right). Focus on NULLs on the left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9673cab9-8475-4625-9694-0d28f4982fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(\"shipment_id\")\n",
    "\n",
    "shipment_df5_str = shipment_df5_str.withColumn(\n",
    "    \"driver_id\",\n",
    "    row_number().over(window_spec) #assign driver_id\n",
    ")\n",
    "display(shipment_df5_str)\n",
    "#Spark SQL does not recognize Python DataFrame variable names.Only aliases created using .alias() can be used in column references.\n",
    "staff = cleanseddf12_str.alias(\"staff\")\n",
    "ship  = shipment_df5_str.alias(\"ship\")\n",
    "\n",
    "orphan_shipments_df = (\n",
    "    staff\n",
    "    .join(\n",
    "        ship,\n",
    "        col(\"staff.staff_id\") == col(\"ship.driver_id\"),\n",
    "        how=\"right\"\n",
    "    )\n",
    "    .filter(col(\"staff.staff_id\").isNull())\n",
    "    .select(\n",
    "        col(\"ship.driver_id\"),\n",
    "        col(\"staff.staff_id\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(orphan_shipments_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273b1d52-8e7a-4a81-b4b6-6ef39a5a33d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Full Outer Join (Reconciliation):\n",
    "- Scenario: A complete audit to find both idle drivers AND unassigned shipments in one view.\n",
    "- Action: Perform a Full Outer Join on shipment_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34a6105-c2c4-486d-9034-a39597a64131",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"is_expedited\":93},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768749088668}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create DataFrame aliases\n",
    "staff_df = cleanseddf12_str.alias(\"staff\")\n",
    "shipments_df = shipment_df5_str.alias(\"ship\")\n",
    "display(shipments_df)\n",
    "shipments_df.printSchema()\n",
    "display(staff_df)\n",
    "staff_df.printSchema()\n",
    "#Perform the FULL OUTER JOIN\n",
    "full_join_df = (\n",
    "    staff_df.join(shipments_df,\n",
    "        col(\"staff.shipment_id\") == col(\"ship.shipment_id\"),how=\"full\")\n",
    ")\n",
    "#Add reconciliation status column\n",
    "recon_df = (\n",
    "    full_join_df\n",
    "    .withColumn(\n",
    "        \"reconciliation_status\",\n",
    "        when(col(\"staff.staff_id\").isNull(), \"UNASSIGNED_SHIPMENT\")\n",
    "        .when(col(\"ship.shipment_id\").isNull(), \"IDLE_DRIVER\")\n",
    "        .otherwise(\"OK\")\n",
    "    )\n",
    ")\n",
    "#select\n",
    "final_reconciliation_df = recon_df.select(\n",
    "    col(\"staff.staff_id\"),\n",
    "    col(\"staff.shipment_id\").alias(\"staff_shipment_id\"),\n",
    "    col(\"ship.shipment_id\").alias(\"ship_shipment_id\"),\n",
    "    col(\"ship.driver_id\"),\n",
    "    col(\"reconciliation_status\")\n",
    ")\n",
    "display(final_reconciliation_df)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "702cd6e1-fe8f-4d89-80a6-d835d42f5c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cartesian/Cross Join (Capacity Planning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc08438-2827-4e80-a860-2d69d7ae10d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_df = cleanseddf12_str.select(\n",
    "    \"staff_id\",\n",
    "    \"first_name\",\n",
    "    \"hub_location\"\n",
    ").alias(\"d\")\n",
    "\n",
    "pending_shipments_df = shipment_df5_str.select(\n",
    "    \"shipment_id\",\n",
    "    \"driver_id\",\n",
    "    \"shipment_cost\"\n",
    ").alias(\"s\")\n",
    "capacity_matrix_df = drivers_df.join(pending_shipments_df, how=\"cross\")\n",
    "display(capacity_matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12add938-d489-4287-94f9-f67e97c10658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- 1.3 Advanced Joins (Semi and Anti)\n",
    "- Left Semi Join (Existence Check):\n",
    "- Scenario: \"Show me the details of Drivers who have at least one shipment.\" (Standard filtering).\n",
    "- Action: staff_df.join(shipments_df, \"shipment_id\", \"left_semi\").\n",
    "- Benefit: Performance optimization; it stops scanning the right table once a match is found.\n",
    "- üîπ What is a LEFT SEMI JOIN?\n",
    "- A Left Semi Join:\n",
    "- Returns only rows from the LEFT table\n",
    "- Keeps a row if at least one matching row exists in the RIGHT table\n",
    "- Does NOT return any columns from the right table\n",
    "- Think of it as:\n",
    "- ‚ÄúFilter the left table based on existence in the right table‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71571e8b-1e66-4cde-aff0-3fb578e7403f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_with_shipments_df = (staff_df.join(shipments_df,  staff_df.staff_id == shipments_df.driver_id,\n",
    "                                           how=\"left_semi\"))\n",
    "\n",
    "display(drivers_with_shipments_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96f2018-2664-4860-9436-6ee7e4e23aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- What is a LEFT ANTI JOIN?\n",
    "- A Left Anti Join:\n",
    "- Returns only rows from the LEFT table\n",
    "- Keeps rows ONLY if NO matching row exists in the RIGHT table\n",
    "- Returns no columns from the right table\n",
    "- Think of it as:\n",
    "- ‚ÄúGive me rows from the left that do NOT exist on the right‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564a8722-d776-4698-bf15-695d2e62bec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_without_shipments_df = (staff_df.join(\n",
    "        shipments_df,\n",
    "        staff_df.staff_id == shipments_df.driver_id,\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(drivers_without_shipments_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6a8108-b086-4463-a380-02c23c05fad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Step 2: VALID lookup (LEFT SEMI JOIN)**\n",
    "- ‚úî Keeps only staff rows\n",
    "- ‚úî Includes staff whose hub_location exists in master\n",
    "- ‚úî No unnecessary columns\n",
    "- ‚úî Stops scanning master after first match\n",
    "- **Step 3: INVALID lookup (LEFT ANTI JOIN)**\n",
    "- ‚úî Returns staff whose hub_location does NOT exist in master\n",
    "- ‚úî Perfect for data quality failure detection\n",
    "- ‚úî Cleaner than LEFT JOIN + IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e748d94-62a8-4211-9deb-9b017c338321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Lookup\n",
    "staff_df = staff_df.alias(\"staff\")          # merged staff dataframe\n",
    "master_city_df = shipment_df.alias(\"master\")  # acts as Master_City_List\n",
    "#Step 2: VALID lookup (hub_location exists in master)\n",
    "valid_staff_df = (\n",
    "    staff_df\n",
    "    .join(master_city_df,staff_df.hub_location == master_city_df.source_city,\n",
    "        how=\"left_semi\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(valid_staff_df)\n",
    "#Step 3: INVALID lookup (hub_location NOT in master)\n",
    "#Use LEFT ANTI JOIN (lookup failure)\n",
    "invalid_staff_df = (\n",
    "    staff_df.join(master_city_df,staff_df.hub_location == master_city_df.source_city,\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    ")\n",
    "display(invalid_staff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b882008-d7d2-457a-8578-2311b7f46612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Lookup & Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41330bab-6b31-4ff9-be4d-0edce7868bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4.** Schema Modeling (Denormalization)**\n",
    "- Denormalization means:\n",
    "- Intentionally combining multiple related tables into a single wide table by duplicating dimension attributes.\n",
    "> - 3.**Design Principle (IMPORTANT)**\n",
    "- Shipments = FACT table\n",
    "- Staff = DIMENSION\n",
    "- Use LEFT JOIN so no shipment is lost\n",
    "- Gold tables are denormalized & analytics\n",
    "> - 2.**How Denormalization is Done (Technically)**\n",
    "- Step-by-step pattern\n",
    "- Choose Fact table (Shipments)\n",
    "- LEFT JOIN all Dimensions (Staff, Vehicle, City, etc.)\n",
    "- Select required columns\n",
    "- Rename columns for business clarity\n",
    "- Persist as Gold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73e658a-bcee-44e0-a46b-0eec671bc1d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Prepare & Alias DataFrames\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "staff_df = cleanseddf12_str.alias(\"staff\")\n",
    "shipments_df = shipment_df5_str.alias(\"ship\")\n",
    "#Step 2: Join Shipments ‚Üí Staff (Driver Dimension)\n",
    "shipment_staff_df = (\n",
    "    shipments_df\n",
    "    .join(\n",
    "        staff_df,\n",
    "        col(\"ship.driver_id\") == col(\"staff.staff_id\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "#Step 3: Flatten into a Gold (Wide) Schema\n",
    "wide_shipment_history_df = shipment_staff_df.select(\n",
    "    # ‚îÄ‚îÄ Shipment facts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    col(\"ship.shipment_id\"),\n",
    "    col(\"ship.order_id\"),\n",
    "    col(\"ship.shipment_date\"),\n",
    "    col(\"ship.shipment_status\"),\n",
    "    col(\"ship.shipment_cost\"),\n",
    "    col(\"ship.shipment_weight_kg\"),\n",
    "    col(\"ship.cargo_type\"),\n",
    "    col(\"ship.payment_mode\"),\n",
    "    col(\"ship.is_expedited\"),\n",
    "    col(\"ship.source_city\"),\n",
    "    col(\"ship.destination_city\"),\n",
    "    col(\"ship.vehicle_type\"),\n",
    "\n",
    "    # ‚îÄ‚îÄ Driver / Staff attributes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    col(\"staff.staff_id\"),\n",
    "    col(\"staff.first_name\").alias(\"driver_first_name\"),\n",
    "    col(\"staff.last_name\").alias(\"driver_last_name\"),\n",
    "    col(\"staff.age\").alias(\"driver_age\"),\n",
    "    col(\"staff.role\").alias(\"driver_role\"),\n",
    "    col(\"staff.hub_location\").alias(\"driver_hub\"),\n",
    "\n",
    "    # ‚îÄ‚îÄ Audit columns ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    col(\"ship.domain\"),\n",
    "    col(\"ship.ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "display(wide_shipment_history_df)\n",
    "#Step 4: Persist as Gold Layer Table\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "wide_shipment_history_df.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"gold.wide_shipment_history\"\n",
    ")\n",
    "spark.sql(\"SHOW TABLES IN gold\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4268cfe-ee79-4aff-9ba2-6fffa2c45734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Windowing (Ranking & Trends)\n",
    "- Source Files:\n",
    "- DF of logistics_source2: Provides hub_location (Partition Key).\n",
    "- logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)\n",
    "- Scenario: \"Who are the Top 3 Drivers by Cost in each Hub?\"\n",
    "- For dense_rank\n",
    "- ‚ÄúI use dense_rank() when business wants to keep ties, even if it returns more than N results.‚Äù\n",
    "- For row_number\n",
    "- ‚ÄúI use row_number() when I need a strict Top-N result per partition.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9544e716-ede3-4369-bc70-017bf4b6537a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Join Staff + Shipments (to get cost per driver)\n",
    "staff_df = cleanseddf12_str.alias(\"staff\")\n",
    "shipments_df = shipment_df5_str.alias(\"ship\")\n",
    "\n",
    "staff_shipments_df = (\n",
    "    staff_df\n",
    "    .join(\n",
    "        shipments_df,\n",
    "        col(\"staff.staff_id\") == col(\"ship.driver_id\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "#Step 2: Aggregate total shipment cost per driver per hub\n",
    "driver_cost_df = (\n",
    "    staff_shipments_df\n",
    "    .groupBy(\n",
    "        col(\"staff.hub_location\"),\n",
    "        col(\"staff.staff_id\"),\n",
    "        col(\"staff.first_name\"),\n",
    "        col(\"staff.last_name\")\n",
    "    )\n",
    "    .agg(\n",
    "        sum(\"ship.shipment_cost\").alias(\"total_shipment_cost\")\n",
    "    )\n",
    ")\n",
    "#Step 3: Define Window (Partition + Order)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank, row_number\n",
    "\n",
    "window_spec = (\n",
    "    Window\n",
    "    .partitionBy(\"hub_location\")\n",
    "    .orderBy(col(\"total_shipment_cost\").desc())\n",
    ")\n",
    "#Step 4A: Use dense_rank() (ties allowed)\n",
    "ranked_dense_df = (\n",
    "    driver_cost_df\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    ")\n",
    "#Filter Top 3\n",
    "top3_dense_df = ranked_dense_df.filter(col(\"dense_rank\") <= 3)\n",
    "display(top3_dense_df)\n",
    "#Step 4B: Use row_number() (strict Top 3)\n",
    "ranked_row_df = (\n",
    "    driver_cost_df\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec))\n",
    ")\n",
    "#Filter Top 3\n",
    "top3_row_df = ranked_row_df.filter(col(\"row_number\") <= 3)\n",
    "display(top3_row_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d5ec284-ddcd-4e49-86c2-2dc46ebb8314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6Ô∏è‚É£ **Analytical Functions ‚Äì Lead / Lag**\n",
    "- Scenario\n",
    "- For each driver, calculate the number of days elapsed since their previous shipment.\n",
    "- Source DF: logistics_shipment_detail_3000.json\n",
    "- Key: driver_id\n",
    "- Ordering column: shipment_date\n",
    "- ‚ÄúIdle time per driver is calculated using lag() over a window partitioned by driver and ordered by shipment date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7beb92ca-59c8-483c-b39b-d39ec52aba5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#idle_days = current_shipment_date ‚àí previous_shipment_date\n",
    "from pyspark.sql.functions import col, to_date\n",
    "shipments_df.select(\"shipment_date\").distinct().show(10, truncate=False)\n",
    "#Convert it to date type\n",
    "shipments_df = shipment_df5_str.withColumn(\n",
    "    \"shipment_date\",\n",
    "    to_date(col(\"shipment_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "#Step 2: Define Window (per driver, ordered by date)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = (\n",
    "    Window\n",
    "    .partitionBy(\"driver_id\")\n",
    "    .orderBy(\"shipment_date\")\n",
    ")\n",
    "#Step 3: Use lag() to get previous shipment date\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "shipments_with_prev_df = shipments_df.withColumn(\n",
    "    \"prev_shipment_date\",\n",
    "    lag(\"shipment_date\").over(window_spec)\n",
    ")\n",
    "#Step 4: Calculate idle days\n",
    "idle_time_df = shipments_with_prev_df.withColumn(\n",
    "    \"idle_days\",\n",
    "    datediff(col(\"shipment_date\"), col(\"prev_shipment_date\"))\n",
    ")\n",
    "idle_time_final_df = idle_time_df.select(\n",
    "    \"driver_id\",\n",
    "    \"shipment_id\",\n",
    "    \"shipment_date\",\n",
    "    \"prev_shipment_date\",\n",
    "    \"idle_days\"\n",
    ")\n",
    "#If each driver truly has one shipment: ,Idle time cannot be computed, NULL is the correct output.\n",
    "display(idle_time_final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df2eb930-1a33-47e0-b8a3-4f496c2c29fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Option 2Ô∏è‚É£ Compute idle time per HUB instead (useful alternative)\n",
    "- If business wants hub activity gaps instead of driver gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9281fb43-b234-46e2-9e96-7d39cb63490f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, datediff, to_date, col\n",
    "\n",
    "df = shipment_df5_str.withColumn(\n",
    "    \"shipment_date\",\n",
    "    to_date(col(\"shipment_date\"))\n",
    ")\n",
    "\n",
    "window_spec = (\n",
    "    Window\n",
    "    .partitionBy(\"source_city\")\n",
    "    .orderBy(\"shipment_date\")\n",
    ")\n",
    "\n",
    "hub_idle_df = df.withColumn(\n",
    "    \"prev_shipment_date\",\n",
    "    lag(\"shipment_date\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"idle_days\",\n",
    "    datediff(col(\"shipment_date\"), col(\"prev_shipment_date\"))\n",
    ")\n",
    "\n",
    "display(hub_idle_df.select(\n",
    "    \"source_city\",\n",
    "    \"shipment_id\",\n",
    "    \"shipment_date\",\n",
    "    \"prev_shipment_date\",\n",
    "    \"idle_days\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c297cfc-7643-4f3f-a7e8-2134831c5287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Set Operations (Union / Intersect / Except)**\n",
    "- Union\n",
    "- ‚ÄúUnion combines datasets vertically.‚Äù\n",
    "- Intersect\n",
    "- ‚ÄúIntersect identifies common rows across datasets.‚Äù\n",
    "- Except\n",
    "- ‚ÄúExcept finds rows missing in another dataset.‚Äù\n",
    "- Semi Join\n",
    "- ‚ÄúSemi joins perform optimized existence checks.‚Äù\n",
    "- Anti Join\n",
    "- ‚ÄúAnti joins detect non-matching records efficiently.‚Äù\n",
    "- 1.Union        ‚Üí combine\n",
    "- 2.Intersect    ‚Üí common\n",
    "- 3.Except       ‚Üí difference\n",
    "- 4.Semi Join    ‚Üí exists\n",
    "- 5.Anti Join    ‚Üí not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1948513d-9cc5-4194-a64a-8ceef8fbae7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_system1 = rawdf1.withColumn(\"data_source\", lit(\"system1\"))  # Legacy\n",
    "df_system2 = rawdf2.withColumn(\"data_source\", lit(\"system2\"))  # Modern\n",
    "#UNION / UNION BY NAME\n",
    "rawdf_merged = df_system1.unionByName(\n",
    "    df_system2,\n",
    "    allowMissingColumns=True\n",
    ")\n",
    "#INTERSECT\n",
    "staff_keys_sys1 = df_system1.select(\"shipment_id\")\n",
    "staff_keys_sys2 = df_system2.select(\"shipment_id\")\n",
    "common_staff_df = staff_keys_sys1.intersect(staff_keys_sys2)\n",
    "display(common_staff_df)\n",
    "#EXCEPT (DIFFERENCE)\n",
    "new_hires_df = staff_keys_sys2.exceptAll(staff_keys_sys1)\n",
    "display(new_hires_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e18a55b1-17d4-400f-a298-f3e281cca1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Grouping & Aggregations (Advanced)\n",
    "> CUBE / ROLLUP generate multiple aggregation levels in a single query, instead of writing separate GROUP BYs.\n",
    "- CUBE\n",
    "- ‚ÄúCube generates all possible subtotals across dimensions, including the grand total.‚Äù\n",
    "- ROLLUP\n",
    "- ‚ÄúRollup generates hierarchical subtotals and a grand total.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf1c1959-be18-4bde-9533-e7691e5d0912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Scenario: The CFO wants a subtotal report at multiple levels:\n",
    "- Total Cost by Hub.\n",
    "- Total Cost by Hub AND Vehicle Type.\n",
    "- Grand Total.\n",
    "- Action: Use cube(\"hub_location\", \"vehicle_type\") or rollup() to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c2cec80-9cd0-4c48-b6a4-18c3574efdb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- GROUP BY (Baseline)\n",
    "| hub_location | vehicle_type | total_cost |\n",
    "| ------------ | ------------ | ---------- |\n",
    "| Pune         | Truck        | 300        |\n",
    "| Pune         | Van          | 150        |\n",
    "| Delhi        | Truck        | 300        |\n",
    "| Delhi        | Van          | 100        |\n",
    "- ROLLUP(hub_location, vehicle_type)\n",
    "- ROLLUP gives hierarchical totals, from left ‚Üí right.\n",
    "| hub_location | vehicle_type | total_cost | Meaning         |\n",
    "| ------------ | ------------ | ---------- | --------------- |\n",
    "| Pune         | Truck        | 300        | Pune + Truck    |\n",
    "| Pune         | Van          | 150        | Pune + Van      |\n",
    "| Pune         | NULL         | 450        | **Total Pune**  |\n",
    "| Delhi        | Truck        | 300        | Delhi + Truck   |\n",
    "| Delhi        | Van          | 100        | Delhi + Van     |\n",
    "| Delhi        | NULL         | 400        | **Total Delhi** |\n",
    "| NULL         | NULL         | 850        | **Grand Total** |\n",
    "- CUBE(hub_location, vehicle_type)\n",
    "- CUBE gives ALL possible combinations of the dimensions.\n",
    "| hub_location | vehicle_type | total_cost | Meaning         |\n",
    "| ------------ | ------------ | ---------- | --------------- |\n",
    "| Pune         | Truck        | 300        | Pune + Truck    |\n",
    "| Pune         | Van          | 150        | Pune + Van      |\n",
    "| Pune         | NULL         | 450        | **Total Pune**  |\n",
    "| Delhi        | Truck        | 300        | Delhi + Truck   |\n",
    "| Delhi        | Van          | 100        | Delhi + Van     |\n",
    "| Delhi        | NULL         | 400        | **Total Delhi** |\n",
    "| NULL         | Truck        | 600        | **Truck Total** |\n",
    "| NULL         | Van          | 250        | **Van Total**   |\n",
    "| NULL         | NULL         | 850        | **Grand Total** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f89f1e-ca1f-4448-8734-5681642849bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Join Staff + Shipments (bring dimensions + metric together)\n",
    "staff_df = cleanseddf12_str.alias(\"staff\")\n",
    "shipments_df = shipment_df5_str.alias(\"ship\")\n",
    "\n",
    "fact_df = (\n",
    "    staff_df\n",
    "    .join(\n",
    "        shipments_df,\n",
    "        col(\"staff.staff_id\") == col(\"ship.driver_id\"),\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "display(fact_df)\n",
    "#Step 2A: Use CUBE (ALL possible subtotal combinations)\n",
    "cube_df = (\n",
    "    fact_df\n",
    "    .cube(\n",
    "        col(\"staff.hub_location\"),\n",
    "        col(\"staff.vehicle_type\")\n",
    "    )\n",
    "    .agg(\n",
    "        sum(col(\"ship.shipment_cost\")).alias(\"total_shipment_cost\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(cube_df)\n",
    "\n",
    "display(cube_df)\n",
    "#Step 2B: Use ROLLUP (Hierarchical subtotals)\n",
    "rollup_df = (\n",
    "    fact_df\n",
    "    .rollup(\n",
    "        col(\"staff.hub_location\"),\n",
    "        col(\"staff.vehicle_type\")\n",
    "    )\n",
    "    .agg(\n",
    "        sum(col(\"ship.shipment_cost\")).alias(\"total_shipment_cost\")\n",
    "    )\n",
    ")\n",
    "display(rollup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdbf31-7a73-44f2-8e74-6c6fcf35d916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fd087852-5e66-4fff-87bc-f1c0c8712d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "final_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"gold.cost_subtotals_by_hub_vehicle\")\n",
    "wide_shipment_history_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"gold.wide_shipment_history\")\n",
    "spark.sql(\"SHOW TABLES IN gold\").show(truncate=False) \n",
    "spark.table(\"gold.wide_shipment_history\").printSchema()\n",
    "display(spark.sql(\"\"\"SELECT * FROM gold.wide_shipment_history\"\"\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee7e8d0-1fbb-4a6c-af39-316374a94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7.Take the copy of the above notebook and try to write the equivalent SQL for which ever applicable."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7712024086149171,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) MyLogistics_BB2_Usecase2_DSL_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
